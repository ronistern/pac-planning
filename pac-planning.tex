%% It is just an empty TeX file.
%% Write your code here.

\def\year{2016}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai16}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\frenchspacing \setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}

%\pdfinfo{Title (Data-and-Model Driven Classical Planning)/Author (Roni Stern, Brendan Juba)}


\setcounter{secnumdepth}{0}



\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{psfig}
\usepackage{subcaption}
\usepackage{tabularx}

\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}

\def\FullBox{\hbox{\vrule width 8pt height 8pt depth 0pt}}
\def\sFullBox{\hbox{\vrule width 6pt height 6pt depth 0pt}}
\newcommand{\qqed}{\(\;\;\;\sFullBox\)}
\newenvironment{proof}{\noindent{\bf Proof:~~}}{\qed}
\newenvironment{proof-of}[1]{\noindent{\bf Proof of {#1}:~~}}{\qed}
\newenvironment{proof-of-claim}{\noindent{\bf Proof:~~}}{\qqed}
\newcommand{\tuple}[1]{\ensuremath{\left \langle #1 \right \rangle }}

%\nocopyright

\newcommand{\comment}[1]{}

\newcommand{\OPEN} {{\textsc{Open}}}

% This is for the agmin
\newcommand{\argmin}{\operatornamewithlimits{argmin}}


\newcommand{\ch}[1]
{{\color{red} #1}}

\newcommand{\qed}{\hfill\ensuremath{\blacksquare}}
\newcommand{\astar}{A$^*$}
\newcommand{\wastar}{WA$^*$}
\newcommand{\arastar}{ARA$^*$}
\newcommand{\pts}{PS}
\newcommand{\dps}{DPS}
\newcommand{\ees}{EES}
\newcommand{\gbfs}{GBFS}
\newcommand{\bees}{BEES}
\newcommand{\beeps}{BEEPS}
\newcommand{\bss}{BSS}
\newcommand{\bssb}{BSS$(B)$}
\newcommand{\bcs}{BCS}
\newcommand{\bcsb}{BCS$(C)$}
\newcommand{\ar}{AR}
\newcommand{\nr}{NR}
\newcommand{\icl}{ICL}
\newcommand{\nrr}{NRR}
\newcommand{\nrrf}{NRR1}
\newcommand{\nrrs}{NRR2}
\newcommand{\open}{\textsc{Open}}
\newcommand{\closed}{\textsc{Closed}}
\newcommand{\focal}{\textsc{Focal}}
\newcommand{\focalc}{\textsc{Focal$(C)$}}
\newcommand{\focals}{Focal Search}
\newcommand{\fmin}{$f_{min}$}


\setlength{\pdfpagewidth}{8.5in} \setlength{\pdfpageheight}{11in}
%\pdfinfo{
%/Title(To Reopen or Not to Reopen)
%/Author(Submission #85)}




\setcounter{secnumdepth}{2}
\newtheorem{theorem}{Theorem}

% Used for commenting
\newcommand{\commenter}[2]{\red{[\uppercase{#1}]:{#2}]}}
\newcommand{\roni}[1]{\commenter{roni}{#1}}
\newcommand{\brendan}[1]{\commenter{brendan}{#1}}



\newcommand{\MEMO}[1]
{ \fbox{
\begin{minipage}[b]{7.9 cm}
#1
\end{minipage}
} }


\begin{document}
\title{Model-Free Contingent Planning}

\author{Submission \#85}

\maketitle

\begin{abstract}%\vspace{-0.2cm}
In this paper we explore the theoretical boundaries of 
performing contingent planning in a setting where 
there is no model of the agent's actions. 
Instead of an action model, a set of trajectories used by successfully executed actions are given and the task is to generate a plan that is guaranteed
to achieve the goal without failing. 
We propose to address this problem by learning a conservative model of the world, in which actions are guaranteed to be applicable, and generating a plan with this conservative model by casting it as a conformant planning problem and using an off-the-shelf conformant planning algorithm. This approach is sound but may be incomplete, not finding plans even in cases when they exist. 
Then, we analyze theoretically the probabilistic relation between the number of observed trajectories and the likelihood that our conservative approach will indeed fail to solve a solvable problem. Our analysis show that the number of trajectories needed scales gracefully. 
\end{abstract}


\section{Introduction}
% Planning problems
In classical planning problems, a model of the acting agent and its relation to the relevant world is given in a formal planning description language, e.g., the classical STRIPS model~\cite{fikes1971strips} or PDDL~\cite{mcdermott1998pddl}. Planning algorithms (planners) using this model to generate a plan for satisfying a given goal condition. 

% Domain modeling is hard and so people try to automated this
Creating a planning domain model for an agent, and in general knowledge engineering, is notoriously hard. %\roni{Brendan, if you have a nice reference for this?} 
Thus, a well-studied approach is to learn a domain model by observing the agent's prior interactions with the environment, and by guiding the agent towards performing exploratory actions. A popular approach to planning without a domain model in {\em reinforcement learning} even skips the model-learning phase and directly learns how to act from observing the agent's past actions and observations~\cite[e.g.]{kearns2002}.  


% We are different: only positive examples, and we really want to succeed
In most prior work, the plans generated by the agent may fail, at which case the agent will replan, possibly refining an underlying domain model it has learned. Thus, the agent learns from both positive and negative examples. In this work we address a different setting, in which execution failure is very costly must be avoided. Furthermore, an agent may have limited computational capabilities, and thus might not have the capability to re-plan after the plan it has tried to execute has failed. 
Consider, for example, a team of nano-bots deployed inside a human body for medical target identification and drug delivery~\cite{cavalcanti2007nanorobot}. Re-planning is likely not possible in such nano-bots, and, of course, failing to cure the disease is undesirable. Thus, the planning task we focus on in this paper is how to find a plan that is guaranteed to achieve the goal, in a setting where a domain model is not available. We call this problem the {\em conformant model-free planning problem}. 


% Our approach 
First, we show how to learn a set of actions that the agent can use from the given trajectories. 
For every such action $a$, we bound the set of predicates that are $a$'s preconditions and effects. This bounded action model is then used  to construct a conformant planning problem such that a solution to it is a solution to our model-free problem. This approach to solve the model-free planning problem 
is sound and can be very efficient, since many conformant planning algorithms exist in the literature~\cite{someConfPlanners}. 
However, it is not complete, as the conformant planning problem might not be solvable even if the underlying model-free planning problem is. Nonetheless, we prove that under some assumptions, the probability of this occurring decreases quasi-linearly with the number of observed trajectories. %\MEMO{I think it is better to say something more accurate than ``quickly''. How do you say in English taht something grows in a $n\cdot logn$ manner? super-linearly?}
%We note that the contributionswhile the proposed solution this short paper lays the theoretical framework for model-free planning

%In this paper we introduce a theoretical framework for learning a partial classical planning model from a given set of observed trajectories, and how to plan using these observed trajectoriesin a way that will ensure the goal is achieved. 


%[[TAKEN FROM "Planning and Learning under Uncertainty" Ph.D. thesis of Sergio Jimenez Celorrio ]] In deterministic and totally observable environments this problem has been well studied. The LIVE system [92] learns operators with quantified variables while exploring the world. The EXPO system [45] refines incomplete planning operators when the monitoring of a plan execution detects a divergence between internal expectations and external observations. [98] learns PRODIGY operators from the execution traces of the resulting solutions or execution failures. The TRAIL system [6] limits its operators to an extended version of Horn clauses so that ILP can be applied. The LOPE system [41] proposed an integrated framework for learning, planning and executing actions



\section{Problem Definition}
The setting we address in this work is a STRIPS planning problem $\Pi=\langle P, A, I, G\rangle$, where $P$ is the set of propositions, $A$ is a set of actions, $I$ is the initial state, and $G$ is the goal condition. States and the goal condition are expressed as conjunctions of propositions from $P$. An action $a\in A$ is defined by its preconditions, add effects, and delete effects, denoted $pre(a)$, $add(a)$, $del(a)$, respectively. The set actions $A$ along with their preconditions and effects is are referred to as the {\em action model} of $\Pi$. We assume that all actions are not parametrized (i.e., they are grounded), and their preconditions and effects are conjunctions of propositions from $P$. 
We leave for future work more complex action models, e.g., that include negative preconditions, conditional effects, stochastic action outcome, and partial observability. 
$\Pi$ will be referred hereinafter as the {\em underlying planning problem}. 


%We assume in this work that the initial state, the goal condition, and action model of the underlying planning problem are all conjunction of propositions. We leave for future work underling problems with conditional effects, negative preconditions, stochastic action outcome, and partial observability. 
The key challenge we address in this paper is that the planner is not given the action model of the underlying planning problem. Instead, it is given a set of {\em trajectories}. 
\begin{definition}[Trajectory]
A trajectory $T=\langle s_1, a_1, s_2, a_2, \ldots, a_{n-1}, s_n\rangle$ is an alternating sequence of states ($s_1,\ldots,s_n$) and actions ($a_1,\ldots,a_n$) that starts and ends with a state. 
\end{definition}
A trajectory represents a successful execution of a sequence of actions by the agent. A set of trajectories may be obtained, for example, by monitoring the acting agent when it is controlled manually by a human operator. The states in the given trajectories are assumed to be fully observable. 


Finally, we can define the {\em conformant model-free planning} problem, which is the focus of this work. 
\begin{definition}[Conformant Model-free planning]
The input to a conformant model-free planning problem is a tuple $\tuple{P,I,G, \mathcal{T}}$, 
where $P$, $I$, and $G$ are the propositions, initial state, and goal condition, respectively, of an underlying planning problem, and $\mathcal{T}$ is the given set of trajectories. The task is to generate plan that
is a solution to the underlying planning problem, 
i.e.,  a a sequence of action that , if is applied to $I$, will result in a state that satisfies $G$.
\label{def:model-free-planning}
\end{definition}


% 1!!!!!!!!!!!!!
\section{Conservative Planning}
%We follow Walsh and Litman's approach
%We follow Wang's~\cite{wang1994learning,wang1995learning} approach for learning a STRIPS model from observation and interactions. 
To solve the conformant model-free planning problem, we propose to learn a conservative action model, and then use it to find sound plans.

Following prior work on learning action models~\cite{wang1995learning,wang1994learning,walsh2008efficientLearning}, we partition every observed trajectory $\tuple{s_1,a_1,s_2,\ldots,s_{n+1}}\in\mathcal{T}$ into a set of {\em action triplets}, where each action triplet is of the form $\langle s_i, a_i, s_{i+1}\rangle$. 
    Let $\mathcal{T}(a)$ be all the action triplets for action $a$. 
    A state $s$ and $s'$ are called pre- and post-state of $a$, respectively, if there is an action triplet $\tuple{s,a,s'}$. %    For an action triplet $\tuple{s,a,s'}$ was call $s$ and $s'$ the pre-state and post-state of $a$, respectivley. 
    Following Walsh and Littman~\shortcite{walsh2008efficientLearning} and Wang~\shortcite{wang1994learning,wang1995learning}, we observe that from the set of trajectories $\mathcal{T}$ we can bound the set of predicates in an action's preconditions and effects, as follows. 
    
    %infer the following:
    \begin{align}
     \emptyset & \subseteq pre(a) \subseteq & \bigcap_{\tuple{s, a, s'}\in \mathcal{T}(a)} s \\
     \bigcup_{\tuple{s, a, s'}\in \mathcal{T}(a)} s'\setminus s  & \subseteq add(a) \subseteq & \bigcap_{\tuple{s, a, s'}\in \mathcal{T}(a)} s' \\
     \bigcup_{\tuple{s, a, s'}\in \mathcal{T}(a)} s\setminus s' & \subseteq del(a) \subseteq & P\setminus \bigcup_{\tuple{s, a, s'}\in \mathcal{T}(a)} s'  
     \label{eq:bounds}
    \end{align}
    We omit a formal proof of the above for space constraints. Informally, the ``upper'' bound on the precondition of $a$ is due to the fact that a predicate cannot be a precondition of $a$ if it is not in every pre-state of $a$. Similarly, the ``upper'' bound on the add and delete effects is because (1) a fact cannot be an add effect of $a$ if it is not in every post-state of $a$, and (2) a fact cannot be a delete effect of $a$ if it exists in a post-state of $a$. 
    The ``lower'' bound on the preconditions of $a$ is the trivial one. 
	As for the ``lower'' bound on the effects, every literal that was in a post-state and not in a pre-state must be an add effect, while every literal that was in the pre-state and not in the post-state must be a delete effect. 
	For convenience, we denote by $pre_\mathcal{T}^u(a)$ the set of ``upper bound'' on the preconditions of $a$ given $\mathcal{T}$. Similary, we denote $add_\mathcal{T}^u(a)$, $add_\mathcal{T}^l(a)$, $del_\mathcal{T}^u(a)$, and $del_\mathcal{T}^l(a)$, as the upper and lower bounds of the add and delete effects. 


Given these bounds on the facts in the pre-conditions and effects, we propose a {\em conservative model-free planning} approach to solve the model-free planning problem. In this approach, we create a conservative planning problem: it has the same predicates, initial state, and goal condition as the model-free planning problem. Its actions are all the actions observed in $\mathcal{T}$. The preconditions of an action are the ``upper'' bound preconditions $pre_\mathcal{T}^u(\cdot)$. The effects are non-deterministic, and consist of any possible combination of add and delete effects in the bounds. 

\subsection{Model-Free Planning via Conformant Planning}
There are several effective fully observable non-deterministic (FOND) planners~\cite{cimatti2003weak}. We aim, however, to find a  solutions to these conservative planning problems that are guaranteed to be applicable. To this end, we create a {\em conformant planning} problem from the conservative planning problem, such that if solvable, will guarantee a solution to the model-free planning problem. Conformant planning can be seen as a middle ground between classical planning and full-blown planning under uncertainty (e.g., MDPs). It supports two forms of uncertainty: uncertainty about the initial state, and non-deterministic action outcomes. % Maybe add a def. of conformant planning and make all the mapping more formal

% Now we can have a conformant planning problem and solve it



%Given these bound on the facts in the pre-conditions and effects, we can devise a {\em conformant planning} problem that, if solveable, will guarantee a solution to the model-free planning problem. Conformant planning can be seen as a middle ground between classical planning and full-blown planning under uncertainty (e.g., MDPs). It supports two forms of uncertainty: uncertainty about the initial state, and non-deterministic action outcome. % Maybe add a def. of conformant planning and make all the mapping more formal

% Mapping model-free to conformant planning
The conformant planning problem that corresponds to a given model-free planning problem $\tuple{P,I,G, \mathcal{T}}$ has the same set of predicates ($P$) and goals ($G$). It has a single possible initial state -- $I$, i.e., there is no uncertainty over the initial state. Its actions are all the actions observed in the observed trajectories $\mathcal{T}$. The preconditions of an action $a$ are $pre_\mathcal{T}^u(a)$. The effects are non-deterministic, and consist of any possible combination of 
add and delete effects that is within the bounds of Equation~\ref{eq:bounds}. A solution to a conformant planning problem is a {\em conformant plan} -- a sequence of actions that is guaranteed to achieve the goal regardless of the uncertainty in the initial condition and in the nondeterministic effects of actions. Thus, a solution to the conformant planning outlined above is necessarily a sound solution to the corresponding model-free planning problem. 


% Something about the difficulty of solving 
This mapping of model-free to contingent planning, and in general our approach of solving model-free planning by solving a conservative planning problem is, however, not complete: if no solution was found for the contingent planning problem it still may be the case the model-free planning has a solution. This has two possible causes: an over-conservative estimate of actions' effects or an over-conservative estimate of actions' preconditions. 

\subsection{Known Effects}

%Following Walsh and Littman~\shortcite{walsh2008efficientLearning}, we consider in the rest of this paper a setting in which action effects are known and the uncertainty only stems from the overconservative set of preconditions. To motivate this special case, observe that $add_\mathcal{T}^u(a)$ can only be incorrect if there is a predicate that is an add effect and it existed in all the pre-states of $a$.     Similarly, $del_\mathcal{T}^u(a)$ is incorrect only if there is a predicate that is a delete effect     and did not exists in any of the pre-states of $a$. While possible, there are many planning domains that prevent this from occuring by verifying that deleted predicates are in the pre-state and added predicates are not already in the pre-state.  
 
 Under certain conditions, $add_\mathcal{T}^u(a)$ 
 and $del_\mathcal{T}^u(a)$ are exactly the effects of $a$. These conditions are:
 (1) if a predicate $f$ is a delete effect of an action $a$ then $f$ is a precondition of $a$, 
 (2) if a predicate $f$ is an add effect of $a$ then $f$ must be mutually exclusive with a precondition of $a$. 
 These conditions are natural in many planning domains and Walsh and Littman~\shortcite{walsh2008efficientLearning} studies this special case in which actions' effects are known and the only uncertaitly is over actions' preconditions. 
 In the rest of this paper we focus on this special case. 
 %Walsh and Littman~\shortcite{walsh2008efficientLearning} considered a special case for learning an action model from trajectories, where action effects are known and the uncertainty only stems from the overconservative set of preconditions. In fact, under certain conditions, $add_\mathcal{T}^u(a)$  and $del_\mathcal{T}^u(a)$ are exactly the effects of $a$. 
 
 % In the rest of this paper we adopt this special case, and motivate it as follows. Observe that $add_\mathcal{T}^u(a)$ can only be incorrect if there is a predicate that is an add effect and it existed in all the pre-states of $a$.     Similarly, $del_\mathcal{T}^u(a)$ is incorrect only if there is a predicate that is a delete effect     and did not exists in any of the pre-states of $a$. While possible, there are many planning domains that prevent this from occuring by verifying that deleted predicates are in the pre-state and added predicates are not already in the pre-state.  
    
    In this case of known effects, there is a much simpler sound and incomplete approach to solving the model-free problem. Instead of compiling the problem to a conformant planning problem, we now compile it to a classical planning problem, having the same initial state, goal state, set of predicates, and actions. The only difference is that the actions' preconditions are the over-conservative sets $pre_\mathcal{T}^u(a)$. It is easy to see that a solution to this problem will also be a solution to the model-free planning problem. However, this approach is incomplete, since $a$'s precondition can be a subset of $pre_\mathcal{T}^u(a)$. This can result in the planner thinking that it cannot apply $a$ in a state that does not contain all the predicates in $pre_\mathcal{T}^u(a)$, even though $a$'s actual precondition would have allowed it to perform $a$ in that state. A key question is how likely it is that this will occur, i.e., that using this approach will not find a solution although one exists. We answer this question in the next section. 



%consider $add_\mathcal{T}^u(a)$ and $del_\mathcal{T}^u(a)$. 


%{\bf Overconservative effects.} A contingent plan is one that achieves the goals for all possible outcomes of the non-deterministic actions. Finding such a plan may not be possible, while finding a plan for the trueFor the add effecs, this means there was a predicate that is an add effect but since it was in all the observed trajectories our 
%An over conservative estimate of an action $a$'s preconditions means that the preconditions $a$ contain fewer predicates than $pre_\mathcal{T}^u(a)$. Thus, there are states where $a$ can be applied but the conformant planner will not consider it. 
%ing problem could not , and thus can be executed in a broader range of states. Consider the latter reason -- an over conservative estimate of the actions effects. For the add effecs, this means there was a predicate that is an add effect but since it was in all the observed trajectories our 


% If we know the effects then it is a classical planning problem, but still incomplete

% What's the probability that is complete


\section{Learning to Perform Model-Free Planning}
%[[Something about the impracticality of the solution above, and how we aim for a more general task, in which we want to maximize probability of success]]
In general, we cannot guarantee that {\em any finite number of trajectories} will suffice to obtain {\em precisely} the underlying action model. This is because, for example, if some action never appears in a trajectory, we may not know its effects; or, if an action is only used in a limited variety of states, it may be impossible to distinguish its preconditions. Consequently, we cannot guarantee a complete solution to the conformant model-free planning problme. However, as the number of trajectories increases, we can hope to learn enough of the actions accurately enough to be able to find plans for most goals in practice. This gives raise to a statistical view of the conformant model-free planning task (Definition~\ref{def:model-free-planning}), which follow the usual statistical view of learning, along the lines of Vapnik and Chervonenkis~\shortcite{vapnik1971} and Valiant~\shortcite{valiant1984}.  %is:
%In this work, we follow the usual statistical view of learning, along the lines of Vapnik and Chervonenkis~\shortcite{vapnik1971} and Valiant~\shortcite{valiant1984}. The main task we consider is:

\begin{definition}[Conformant Model-Free Learning to Plan Task] We suppose that there is an arbitrary, unknown (prior) probability distribution $D$ over triples of the form $\tuple{I,G,T}$, where $I$ is a state, $G$ is a goal condition, and $T$ is a trajectory that starts in $I$ and ends in a state that statisfies $G$, 
and all trajectories are applicable in a fixed action model. In the conformant model-free learning-to-plan task, we are given a set of triplets $\tuple{I^{(1)},G^{(1)},T^{(1)}},\ldots,\tuple{I^{(m)},G^{(m)},T^{(m)}}$ drawn independently from $D$, and we are given as input $T^{(1)},\ldots,T^{(m)}$, and a new initial state and goal $(I,G)$ for some $(I,G,T)$ drawn from $D$. We must either output a plan for $\langle P,A,I,G\rangle$ or, with probability at most $\epsilon$, assert that no plan can be found.
%In the model-free planning task, $(I^{(1)},G^{(1)},T^{(1)}),\ldots,(I^{(m)},G^{(m)},T^{(m)})$ have been drawn independently from $D$, and we are given as input $T^{(1)},\ldots,T^{(m)}$, and a new initial state and goal $(I,G)$ for some $(I,G,T)$ drawn from $D$. We must either output a plan for $\langle P,A,I,G\rangle$ or, with probability at most $\epsilon$, assert that no plan can be found.
\end{definition}

We stress that $D$ is arbitrary, and in particular the conditional distribution
of trajectories given a start and goal state, $D(T|I,G)$, is completely 
arbitrary. Even if $D(T|I,G)$ produces a deterministic outcome $T(I,G)$, this
still captures, for example, the distribution of trajectories obtained
by running some (unknown, sophisticated) planning algorithm on input $(I,G)$, or
more generally a nondeterministic, adversarial choice of trajectory $T(I,G)$ to 
be paired with the start state $I$ and goal $G$.


%We note that this task is conservative in the sense that when an algorithm solving it outputs a plan, the plan is required to be correct. The algorithm may only make errors by failing to find a plan when one exists. %\section{Theoretical Guarantees and Limitations} How many trajectories do we need to guarantee that every plan can be generated?

A key question is how our certainty that a plan can be generated for a new start and goal state grows with the number of trajectories. 


\begin{theorem}
%With probability $1-\delta$, $\frac{(2ln 2)|A|}{\epsilon}(|P|+\log |A|+\log\frac{2}{\delta})$ trajectories suffice for solving the model-free planning problem.
%task of learning to plan.
$\frac{(2ln 2)|A|}{\epsilon}(|P|+\log |A|+\log\frac{2}{\delta})$ trajectories suffice for solving the model-free planning problem
with probability $1-\delta$. 
\roni{MUST FIX THE NAMES OF THE PROBLEMS}
\end{theorem}
%\begin{proof}
%\MEMO{Question \#1: There is a conceptual hop that I am missing. I understand that there is a distribution of start-goal pairs. But I don't understand the meaning of a distribution over trajectories. A trajectory is not something that follows some distribution, it is something that is computed by some planning process.}
\begin{proof}
{\bf Outline.} First, we first prove that the set of actions that exists in the observed trajectories is sufficient to solve a randomly drawn problem with high probability (Lemma~\ref{lem:sufficientActions}). Then, we show that under some conditions of the learned preconditions, they learned actions and preconditions enable finding a plan with high probability (Lemma~\ref{lem:plan-existance}). Finally, we show that these conditions indeed hold when using  conservative model-free planning. 
%although the set of preconditions we assume is conservative, it is still sufficient to enable finding a plan with high probability. 

\begin{lemma}
Let $A_\epsilon$ be the set of actions that appear in trajectories sampled from $D$ with probability at least $\frac{\epsilon}{2|A|}$ 
and let $A(\mathcal{T})$ be the set of actions that appeared in {\em any} trace. 
The probability that any action $a\in A_\epsilon$ does not appear in $A(\mathcal{T})$ is at most $\delta/2$.
\label{lem:sufficientActions}
\end{lemma}
\begin{proof-of-claim}
%Let $A_\epsilon$ be the set of actions that appear in trajectories sampled from $D$ with probability at least $\frac{\epsilon}{2|A|}$%, let $A''$ be the set of actions that appear with probability at least $\frac{\epsilon}{4|A|}$, 
%and let $A(\mathcal{T})$ be the set of actions that appeared in {\em any} trace. 
%\MEMO{Question \#2: So $A'$ is the set of actions used in  trajectories in general (with non-negligable probability), and $A(\mathcal{T})$ are the actions in $\mathcal{T}$?}
%\MEMO{Question \#3: I think we need to assume that the planning process that generated the trajectories is complete.}
%We first observe that the probability that any action $a\in A'$ does not appear in $A(\mathcal{T})$ is at most $\delta/2$: indeed, fix any action $a\in A'$. We note that $a$ is not invoked in each trajectory drawn from $D$ with probability at most $1-\epsilon/2|A|$. 
By definition, the probability that an action in $A_\epsilon$ does not exist in a trajectory drawn from $D$ is $1-\frac{\epsilon}{2|A|}$. Since the observed trajectories $\mathcal{T}$ are drawn independently from $D$ we have that the probability that $a\notin A(\mathcal{T})$ is $(1-\frac{\epsilon}{2|A|})^m$. 
Using the inequality $1-x\leq e^{-x}$ and 
the fact that 
$m>\frac{(2\ln 2)|A|}{\epsilon}\log\frac{2|A|}{\delta}$, we have that the probability that $a\notin A(\mathcal{T})$ is at most $\delta/2$. Hence, by a union bound over $a\in A_\epsilon$ (noting $|A_\epsilon|\leq |A|$), $A_\epsilon\subseteq A(\mathcal{T})$ with probability $1-\delta/2$ as needed.
\end{proof-of-claim}


%Thus, using $1+x\leq e^x$, the probability that $a\notin A(\mathcal{T})$ is less than $\delta/2|A|$, and hence by a union bound over $a\in A'$ (noting $|A'|\leq |A|$), $A'\subseteq A(\mathcal{T})$ with probability $1-\delta/2$ as needed.
%$A(\mathcal{A})$ is will not appear in any observed trajectory Consider an action $a\in A_\epsilon$.
%\MEMO{Question \#4: I think you meant above and below $\frac{\epsilon}{2|A|}$ and not $\epsilon/2|A|$}
%Thus, since our trajectories are drawn independently from $D$, the probability that $a$ does not appear in $A(\mathcal{T})$ is $(1-\epsilon/2|A|)^m$ where $m > \frac{(2\ln 2)|A|}{\epsilon}\log\frac{2|A|}{\delta}$. 
%\MEMO{Question \#4: I think there's an extra $|A|$ there: the $\log\frac{2|A|}{\delta}$ should be $\log\frac{2}{\delta}$ Please verify.} My mistake
%Thus, using $1+x\leq e^x$, the probability that $a\notin A(\mathcal{T})$ is less than $\delta/2|A|$, and hence by a union bound over $a\in A'$ (noting $|A'|\leq |A|$), $A'\subseteq A(\mathcal{T})$ with probability $1-\delta/2$ as needed.

%Let $A'$ be the set of actions that appear in trajectories sampled from $D$ with probability at least $\frac{\epsilon}{2|A|}$%, let $A''$ be the set of actions that appear with probability at least $\frac{\epsilon}{4|A|}$, and let $A(\mathcal{T})$ be the set of actions that appeared in {\em any} trajectory. \roni{Is $A(\mathcal{T})$ referring to any trajectory or only the observed ones?} We first show that the probability that any action $a\in A'$ does not appear in $A(\mathcal{T})$ is at most $\delta/2$. \roni{I am always confused with the term {\em any}. Do you mean that the probability that $A(\mathcal{T})\cap A'=\emptyset$ is at most $\delta/2$?}For any action $a\in A'$, it will not be invoked in a trajectory drawn from $D$ with probability at most $1-\frac{\epsilon}{2|A|}$. 
%: indeed, fix any action $a\in A'$. We note that $a$ is not invoked in each trajectory drawn from $D$ with probability at most $1-\epsilon/2|A|$. 
%: indeed, fix any action $a\in A'$. We note that $a$ is not invoked in each trajectory drawn from $D$ with probability at most $1-\epsilon/2|A|$. Thus, since our trajectories are drawn independently from $D$, the probability that $a$ does not appear in $A(\mathcal{T})$ is $(1-\frac{\epsilon}{2|A|})^m$. Thus, using $1+x\leq e^x$ inequality, the probability that $a\notin A(\mathcal{T})$ is less than ${e^{-\frac{\epsilon}{2|A|}}}^m$. Since $m\geq \frac{(2ln 2)|A|}{\epsilon}(|P|+\log |A|+\log\frac{2}{\delta})$, we have that the probability that $a\notin A(\mathcal{T})$ is less than less than $\delta/2|A|$, and hence by a union bound over $a\in A'$ (noting $|A'|\leq |A|$), $A'\subseteq A(\mathcal{T})$ with probability $1-\delta/2$ as needed.

%$\frac{(2ln 2)|A|}{\epsilon}(|P|+\log |A|+\log\frac{2}{\delta})$


%is $(1-\epsilon/2|A|)^m$ where $m > \frac{(2\ln 2)|A|}{\epsilon}\log\frac{2|A|}{\delta}$.
%is $(1-\frac{\epsilon}{2|A|})^m$ where $m > \frac{(2\ln 2)|A|}{\epsilon}\log\frac{2|A|}{\delta}$. Thus, using $1+x\leq e^x$, the probability that $a\notin A(\mathcal{T})$ is less than $\delta/2|A|$, and hence by a union bound over $a\in A'$ (noting $|A'|\leq |A|$), $A'\subseteq A(\mathcal{T})$ with probability $1-\delta/2$ as needed.
We will call an assignment of preconditions {\em unsafe} (for $A(\mathcal{T})$) if at least one term in at least one of the preconditions for at least one of the actions is missing. We will call an assignment of preconditions {\em inadequate} if, with probability at least $\epsilon/2$ over trajectories $T$ sampled from $D$, there is an action $a\in A_\epsilon$ and a state where $a$ is invoked in $T$ such that the precondition we assign to $a$ is not satisfied. 

\begin{lemma}
%If the preconditions of the learned action model are neither unsafe nor inadequate, then for a start-goal pair $(I,G)$ sampled from $D$, there is a plan for $(I,G)$ that only uses actions in $A_\epsilon$ with probability of at least  $1-\epsilon/2$.
If $A_\epsilon\subseteq A(\mathcal{T})$ and the preconditions of the learned action model are neither unsafe nor inadequate, then for a start-goal pair $(I,G)$ sampled from $D$, with probability $1-\epsilon$ there is a plan for $(I,G)$ that only uses actions in $A(\mathcal{T})$, that in particular will be found by our conservative model-free planner.
\label{lem:plan-existance}
\end{lemma}
\begin{proof-of-claim}
%We prove Lemma~\ref{lem:plan-existance} in two steps: (1) showing that there exists a plan using only actions in $A_\epsilon$ with probabiltiy higher than $1-\epsilon/2$, and (2) showing that the conservative preconditions are satisfied in that plan with probability higher than (1) only uses actions in $A_\epsilon$ and (2) the conservative preconditions learned from $\mathcal{T}$ for these actions are satisfied. By negation, assume that the probability of sampling a start-goal pair $(I,G)$ for which  all trajectories use an action not in $A_\epsilon$ is higher than $\epsilon/2$. This means that the probability of sampling a trajectory for such a start-goal pairs is also higher than $\epsilon/2$. However, the probability of sampling a trajectory that uses an action not in $A_\epsilon$ is at most  $|A|\cdot\frac{\epsilon}{2|A|}=\epsilon/2$ (by definition of $A_\epsilon$).
First, observe that for an assignment of preconditions to actions that is neither unsafe for $A(\mathcal{T})$ nor inadequate for $A_\epsilon$, then for $(I,G)$ taken from $(I,G,T)$ sampled from $D$, with probability $1-\epsilon/2$ there is a plan for $(I,G)$ that only uses actions in $A_\epsilon$: in particular for the full triple $(I,G,T)$, the (unknown) $T$ uses an action outside $A_\epsilon$ with probability at most $|A|\cdot\frac{\epsilon}{2|A|}=\epsilon/2$.
Likewise since the preconditions are not inadequate for $A_\epsilon$, with probability $1-\epsilon/2$ the specified preconditions are satisfied on all of the states in which the corresponding action is used in the unknown trajectory; thus, by a union bound, we find that with probability $1-\epsilon$, the plan in the trajectory is a solution to the planning task in which the actions are restricted to $A_\epsilon\subseteq A(\mathcal{T})$.
Moreover, since the assignment of preconditions to actions is not unsafe, if we only use actions in $A(\mathcal{T})$, whenever our precondition for $a\in A(\mathcal{T})$ is satisfied for a state in our trajectory, the real precondition is also satisfied. 
\end{proof-of-claim}


%Observe that if we can find an assignment of preconditions to actions that is neither unsafe for $A(\mathcal{T})$ nor inadequate for $A_\epsilon$, then for $(I,G)$ sampled from $D$, with probability $1-\epsilon/2$ there is a plan for $(I,G)$ that only uses actions in $A_\epsilon$ since in particular, the corresponding, unknown trajectory uses an action outside $A_\epsilon$ with probability at most $|A|\cdot\frac{\epsilon}{2|A|}=\epsilon/2$, and likewise since the preconditions are not inadequate for $A'$, with probability $1-\epsilon/2$ the specified preconditions are satisfied on all of the states in which the corresponding action is used in the unknown trajectory; thus, with probability $1-\epsilon$, the plan in the trajectory is a solution to the planning task in which the actions are restricted to $A_\epsilon\subseteq A(\mathcal{T})$ (where containment holds with probability $1-\delta/2$, as established above). Moreover, since the assignment of preconditions to actions is not unsafe, if we only use actions in $A(\mathcal{T})$, whenever our precondition for $a\in A(\mathcal{T})$ is satisfied for a state in our trajectory, the real precondition is also satisified. 

It thus suffices to argue that the algorithm finds an assignment of preconditions to actions that is neither unsafe for $A(\mathcal{T})$ nor inadequate for $A_\epsilon$. We first observe that there are $2^{|A||P|}$ candidate preconditions over $|P|$ for our $|A|$ actions. Now, the subset $BAD$ of these assignments of preconditions that are inadequate for at least one action $a\in A_\epsilon$ in particular has size at most $2^{|A||P|}$. We note that any particular inadequate assignment cannot be produced as output by the algorithm if we draw a trajectory in which for one of the actions in $a\in A_\epsilon$ the precondition assigned to $a$ by that inadequate assignment is not satisfied on one of the states in the trajectory in which $a$ is invoked. Note that by the definition of inadequacy, for each example trajectory drawn, such a state appears with probability at least $\epsilon/2$. Therefore, the probability that no state appears in our $m$ independently drawn trajectories is at most $(1-\epsilon/2)^m$. Concretely, since $m\geq\frac{(2ln 2)|A|}{\epsilon}(|P|+\log\frac{2|A|}{\delta})>\frac{2\ln 2}{\epsilon}(|A||P|+\log\frac{2}{\delta})$, this is at most $\delta/2^{|A||P|+1}$. Thus, by a union bound over this set of inadequate assignments $BAD$, the probability that any inadequate assignment of preconditions could be output is at most $\delta/2$.


%We next observe that by a Chernoff bound, the probability that an action $a\in A'$ does not appear in at least an $\epsilon/3|A|$-fraction of the $m$ example trajectories is at most $\exp(-\frac{1}{2}36m\frac{\epsilon}{2|A|})<\delta/2|A|$, and that the probability that an action $a\notin A''$ appears in at least a $\epsilon/3|A|$-fraction of the example trajectories is at most $\exp(-\frac{1}{3}144m\frac{\epsilon}{4|A|})<\delta/2|A|$. Thus by a union bound over the (at most $|A|$) actions in $A'$ or outside $A''$, with probability $1-\delta/2$, all of the actions in $A'$ appear in sufficiently many of the trajectories that our estimates are adequate and no action outside $A''$ appears so often.

We finally note that the algorithm cannot produce unsafe preconditions for $A(\mathcal{T})$: indeed, observe that the preconditions are satisfied for every action invoked in every trajectory obtained by the algorithm. Since the algorithm includes all of the literals that are satisfied on all of the states in which that action was invoked, in particular it includes all of the literals that actually appear in the precondition. Thus, with probability $1-\delta$, the algorithm indeed produces an assignment of preconditions that is neither unsafe for $A(\mathcal{T})$ nor inadequate for $A_\epsilon$, as needed.
\end{proof}


\section{Related Work}
This work focuses on a simpler setting in which there is full state observability, actions have deterministic effects, and there are no conditional effects. Jim{\'e}nez et al.~\shortcite{jimenez2012review} review the state-of-the-art in learning action models for this setting and other more complex settings (partial observability and non-deterministic action outcomes).


Mour{\~{a}}o et al.~\shortcite{mourao2012learning} addressed the problem of learning STRIPS actions given a set of observed trajectories. In their setting, the observed trajectories consist of cases in which the agent tried to perform an action and failed, while in our case the observations are only successful trajectories. Also, we assume full observability while they considered partial and noisy observations of the states in the trajectories. Their approach was to use machine learning methods to predict the outcome of an action and to predict whether it is applicable. Consequently, the plan generated by using this model can fail. By contrast, we aim for a plan that is guaranteed to work. Another similarity of this paper to the paper Mour{\~{a}}o et al.~\shortcite{mourao2012learning} 

Also related is the work of Konidaris et al.~\shortcite{konidaris2014constructing} who showed how to learn a STRIPS model that provides a useful high-level plan for a continuous world. 


The aforementioned work of Wang~\shortcite{wang1995learning,wang1994learning} is, of course, very related to our work. 
She proposed a process for learning a STRIPS model that includes learning an action model. Unlike our work, she discussed how to refine that action model by interacting with the world, including learning from failed 
executions of actions.  We require that the plan generated by the learned actions {\em must} work. This is suitable for setting where plan failure is unacceptable, e.g., where failure corresponds to physical harm to the acting agent.  


Levine and DeJong~\shortcite{levine2006explanation} proposed how to learn control operators 
from background knowledge and experimentations. A similar approach is taken in many reinforcement learning algorithms. Again, our task is different, in that we do not allow experimentation and aim for a plan that is guaranteed to be sound.

Jim{\'e}nez et al.~\shortcite{jimenez2013integrating} also studied how to integrate learning of planning operators and planning. Their approach included an option to experiment with planning rules that are probably correct, while we aim for a provably sound plan. 

Walsh and Littman~\shortcite{walsh2008efficientLearning} also discuss the problem of learning STRIPS operators from observed trajectories. They too interleave planning and execution: they learn an action model, find a plan for a given planning problem, and update that action model by the success or failure of the generated plan. Unlike other work, they also provide a theoretical bounds on the sample complexity -- the number of interactions needed in order to guarantee that the resulting planner is sound and complete. By contrast, we do not assume any planning and execution loop, and we aim for a planning algorithm that is guaranteed to be sound, at the cost of completeness. 
This affects their approach to learning: they attempted to follow an optimistic assumption about the preconditions and effects of the learned actions, in an effort to identify inaccuracies in their action model. By contrast, we are forced to take a pessimistic approach, as we aim for a successful execution of the plan rather than information gathering to improve the action model. 


\section{Conclusions}
This paper deals with a planning problem in which the planner agent does not know the agent's actions. Instead of an action model, the planner is given a set of observed trajectories of successfully executed plans. 
In this setting we introduce the {\em model-free planning} problem, in which the task is to find a plan that is guaranteed to reach the goal, i.e., there is no tolerance for execution failure. This type of problem is important in cases where failure is costly or in cases where the agent has no capability to replan during execution. 
We show how to use the given set of trajectories to learn
information about the agent's actions, including bounds on the preconditions and effects of actions used in the given trajectories. Then, we propose a solution to the model-free problem that is based on a translation to conformant planning. The solution is sound but is not complete, as it may fail to find a solution even if one exists. 
Then, we prove that under some assumptions the likelihood of finding a solution with this approach grows linearly with the number of predicates and quasi-linearly with the number of actions. 
Future directions for model-free planning include studying how to relax the simplifying assumptions made about the action's effects, how to consider richer underlying planning models including parametric actions, conditional effects, stochastic action outcomes, and partial observability.  



\newpage

\bibliography{library}
\bibliographystyle{aaai}

\end{document}

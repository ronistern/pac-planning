%% It is just an empty TeX file.
%% Write your code here.

\def\year{2016}\relax
%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai16}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\frenchspacing \setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}

%\pdfinfo{Title (Data-and-Model Driven Classical Planning)/Author (Roni Stern, Brendan Juba)}


\setcounter{secnumdepth}{0}



\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{psfig}
\usepackage{subcaption}
\usepackage{tabularx}

\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\newtheorem{definition}{Definition}


\def\FullBox{\hbox{\vrule width 8pt height 8pt depth 0pt}}
\def\sFullBox{\hbox{\vrule width 6pt height 6pt depth 0pt}}
\newcommand{\qqed}{\(\;\;\;\sFullBox\)}
\newenvironment{proof}{\noindent{\bf Proof:~~}}{\qed}
\newenvironment{proof-of}[1]{\noindent{\bf Proof of {#1}:~~}}{\qed}
\newcommand{\tuple}[1]{\ensuremath{\left \langle #1 \right \rangle }}

%\nocopyright

\newcommand{\comment}[1]{}

\newcommand{\OPEN} {{\textsc{Open}}}

% This is for the agmin
\newcommand{\argmin}{\operatornamewithlimits{argmin}}


\newcommand{\ch}[1]
{{\color{red} #1}}

\newcommand{\qed}{\hfill\ensuremath{\blacksquare}}
\newcommand{\astar}{A$^*$}
\newcommand{\wastar}{WA$^*$}
\newcommand{\arastar}{ARA$^*$}
\newcommand{\pts}{PS}
\newcommand{\dps}{DPS}
\newcommand{\ees}{EES}
\newcommand{\gbfs}{GBFS}
\newcommand{\bees}{BEES}
\newcommand{\beeps}{BEEPS}
\newcommand{\bss}{BSS}
\newcommand{\bssb}{BSS$(B)$}
\newcommand{\bcs}{BCS}
\newcommand{\bcsb}{BCS$(C)$}
\newcommand{\ar}{AR}
\newcommand{\nr}{NR}
\newcommand{\icl}{ICL}
\newcommand{\nrr}{NRR}
\newcommand{\nrrf}{NRR1}
\newcommand{\nrrs}{NRR2}
\newcommand{\open}{\textsc{Open}}
\newcommand{\closed}{\textsc{Closed}}
\newcommand{\focal}{\textsc{Focal}}
\newcommand{\focalc}{\textsc{Focal$(C)$}}
\newcommand{\focals}{Focal Search}
\newcommand{\fmin}{$f_{min}$}


\setlength{\pdfpagewidth}{8.5in} \setlength{\pdfpageheight}{11in}
%\pdfinfo{
%/Title(To Reopen or Not to Reopen)
%/Author(Submission #)}




\setcounter{secnumdepth}{2}
\newtheorem{theorem}{Theorem}

% Used for commenting
\newcommand{\commenter}[3]{$[$\uppercase{#1}#2:#3$]$  \\}
\newcommand{\ariel}[2]{\commenter{ariel}{#1}{#2}}
\newcommand{\roni}[2]{\commenter{roni}{#1}{#2}}
\newcommand{\vitali}[2]{\commenter{vitali}{#1}{#2}}

\newcommand{\MEMO}[1]
{ \fbox{
\begin{minipage}[b]{7.9 cm}
#1
\end{minipage}
} }


\begin{document}
\title{Model-Free Contingent Planning}

\author{XXX}

\maketitle

\begin{abstract}%\vspace{-0.2cm}
In this paper we explore the theoretical boundaries of 
performing contingent planning in setting where 
there is no model of the agent's actions. 
Instead, a set of trajectories used by successfully executed actions are given and the task is to generate a plan that is guaranteed
to achieve the goal without failing. 
We propose to address this problem by learning a conservative model of the world, 
in which actions are guaranteed to be applicable, and generating a plan with this conservative model. 
This paper studies this conservative approach, answering key questions such as 
when this approach is applicable and when the possiblity of failure is unavoidable, 
when it is complete with high likelihood, 
and what can be said about the cost of the resulting plan. 
\end{abstract}


\section{Introduction}
% Planning problems
In classical planning problems, a model of the acting agent and its relation to the relevant world is given in the form of some of planning description language, e.g., the classical STRIPS model~\cite{fikes1971strips} or PDDL~\cite{mcdermott1998pddl}. Planning algorithms (planners) using this model to generate a plan for satisfying a given goal condition. 

% Domain modeling is hard and so people try to automated this
Creating a planning domain model for an agent, and in general knowledge engineering, is notoriously hard. %\roni{Brendan, if you have a nice reference for this?} 
Thus, a well-studied approach is to learn a domain model by observing the agent's prior interactions with the environment, and by guiding the agent towards performing exploratory actions. A popular approach in the {\em reinforcement learning} to planning without a domain model even skips the model-learning phase and directly learns how to act from observing the agent's past actions and observations~\cite{}.  


% We are different: only positive examples, and we really want to succeed
In most prior work, the plans generated by the agent may fail, at which case the agent will replan, possibly refining an underlying domain model it has learned. Thus, the agent learns from both positive and negative examples. In this work we address a different setting, in which execution failure is very costly must be avoided. Moreover, agents can have limited computational capabilities, and thus do not have the capability to re-plan after the plan it has tried to execute has failed. 
Consider, for example, a team of nano-bots deployed inside a human body to find and cure some disease~\cite{gal?}. Re-planning is likely not possible in such nano-bots, and, of course, failing to cure the disease is undesirable. Thus, the planning task we focus on in this paper is how to find a plan that is guaranteed to achieve the goal, 
in a setting where a domain model is not available. We call this problem the {\em model-free planning problem}. 


% Our approach 
First, we show how to learn from the given trajectories a set of actions
that the agent can use. For every such action $a$, we bound the set of predicates that
are $a$'s preconditions and effects. This bounded action model is then used 
to constract a conformant planning problem such that a solution to 
it is a solution to our model-free problem. This approach to solve the model-free planning problem 
is sound and can be very efficient, since many conformant planning algorithms exist in the literature~\cite{someConfPlanners}. 
However, it is not complete, as the conformant planning problem might not be solvable even if the underlying model-free planning problem is. 
Nonetheless, we prove that under some assumptions, the probability of this occuring decreases quickly with the number of observed trajectories. \roni{Say something better than quickly}
%We note that the contributionswhile the proposed solution this short paper lays the theoretical framework for model-free planning

%In this paper we introduce a theoretical framework for learning a partial classical planning model from a given set of observed trajectories, and how to plan using these observed trajectoriesin a way that will ensure the goal is achieved. 


%[[TAKEN FROM "Planning and Learning under Uncertainty" Ph.D. thesis of Sergio Jimenez Celorrio ]] In deterministic and totally observable environments this problem has been well studied. The LIVE system [92] learns operators with quantified variables while exploring the world. The EXPO system [45] refines incomplete planning operators when the monitoring of a plan execution detects a divergence between internal expectations and external observations. [98] learns PRODIGY operators from the execution traces of the resulting solutions or execution failures. The TRAIL system [6] limits its operators to an extended version of Horn clauses so that ILP can be applied. The LOPE system [41] proposed an integrated framework for learning, planning and executing actions



\section{Problem Definition}
The setting we address in this work is a STRIPS planning problem $\Pi=\langle P, A, I, G\rangle$, where $P$ is the set of predicates, $A$ is a set of actions, $I$ is the initial state, and $G$ is the goal condition. We assume here that states in the world as well as the goal condition are defined as a conjunction of predicates from $P$. An action $a\in A$ is defined by its preconditions, add effects, and delete effects, denoted $pre(a)$, $add(a)$, $del(a)$, respectively. 
We do not consider conditional effects in this work, and assume that 
$pre(a)$, $add(a)$, and $del(a)$ are all conjunctions of literals. Lastly, we assume that all actions and predicates are grounded, and that there is no negative preconditions.  


In the planning problem we deal with in this paper, which we refer to as {\em model-free planning}, we do not know the capabilities of the action agent directly. Instead, we are given a set of {\em trajectories}. 
\begin{definition}[Trajectory]
A trajectory $T=\langle s_1, a_1, s_2, a_2, \ldots, a_{n-1}, s_n\rangle$ is an alternating sequence of states ($s_1,\ldots,s_n$) and actions ($a_1,\ldots,a_n$) that starts and ends with a state. 
\end{definition}
This work focuses on a simpler setting in which there is full state observability, actions have deterministic effects, and there are no conditional effects. Jim{\'e}nez et al.~\shortcite{jimenez2012review} review the state-of-the-art in learning action models for this setting and other more complex settings (partial observability and non-deterministic action outcomes). 


The model-free planning problem is defined as follows.
\begin{definition}[Model-free planning]
The model-free planning problem is defined by the tuple $\tuple{P,I,G, \mathcal{T}}$, 
where $P$, $I$, and $G$ are the predicates, initial state, and goal condition, and $\mathcal{T}$ is a set of trajectories. The task is to generate plan that will achieve $G$, i.e., a sequence of action that, if applied to $I$, and results in a state that satisfies $G$.
\label{def:model-free-planning}
\end{definition}


\section{Conservative Model-Free Planning}
%We follow Walsh and Litman's approach
%We follow Wang's~\cite{wang1994learning,wang1995learning} approach for learning a STRIPS model from observation and interactions. 
Following prior work on learning action models~\cite{wang1995learning,wang1994learning,walsh2008efficientLearning}, we partition every observed trajectory $\tuple{s_1,a_1,s_2,\ldots,s_{n+1}}\in\mathcal{T}$ into a set of {\em action triplets}, where each action triplet is of the form $\langle s_i, a_i, s_{i+1}\rangle$. 
    Let $\mathcal{T}(a)$ be all the action triplets for action $a$. 
    A state $s$ and $s'$ are called pre- and post-state of $a$, respectively, if there is an action triplet $\tuple{s,a,s'}$. %    For an action triplet $\tuple{s,a,s'}$ was call $s$ and $s'$ the pre-state and post-state of $a$, respectivley. 
    Following Walsh and Littman~\shortcite{walsh2008efficientLearning} and Wang~\shortcite{wang1994learning,wang1995learning}, we observe that from the set of trajectories $\mathcal{T}$ we can bound the set of predicates in an action's preconditinos and effects, as follows. 
    
    %infer the following:
    \begin{align}
     \emptyset & \subseteq pre(a) \subseteq & \bigcap_{\tuple{s, a, s'}\in \mathcal{T}(a)} s \\
     \bigcup_{\tuple{s, a, s'}\in \mathcal{T}(a)} s'\setminus s  & \subseteq add(a) \subseteq & \bigcap_{\tuple{s, a, s'}\in \mathcal{T}(a)} s \\
     \bigcup_{\tuple{s, a, s'}\in \mathcal{T}(a)} s\setminus s' & \subseteq del(a) \subseteq & P\setminus \bigcup_{\tuple{s, a, s'}\in \mathcal{T}(a)} s'  
     \label{eq:bounds}
    \end{align}
    We omit a formal proof of the above for space constraints. Informally, the ``upper'' bound on the precondition of $a$ is due to the fact that a predicate cannot be a precondition of $a$ if it is not in every pre-state of $a$. Similarly, the ``upper'' bound on the add and delete effects is because (1) a fact cannot be an add effect of $a$ if it is not in every post-state of $a$, and (2) a fact cannot be a delete effect of $a$ if it exists in a post-state of $a$. 
    The ``lower'' bound on the preconditions of $a$ is the trivial one. 
	As for the ``lower'' bound on the effects, every literal that was in a post-state and not in a pre-state must be an add effect, while every literal that was in the pre-state and not in the post-state must be a delete effect. 
	For convenience, we denote by $pre_\mathcal{T}^u(a)$ the set of ``upper bound'' on the preconditions of $a$ given $\mathcal{T}$. Similary, we denote $add_\mathcal{T}^u(a)$, $add_\mathcal{T}^l(a)$, $del_\mathcal{T}^u(a)$, and $del_\mathcal{T}^l(a)$, as the upper and lower bounds of the add and delete effects. 


% Now we can have a conformant planning problem and solve it
Given these bound on the facts in the pre-conditions and effects, we can devise a {\em conformant planning} problem that, if solveable, will guarantee a solution to the model-free planning problem. Conformant planning can be seen as a middle ground between classical planning and full-blown planning under uncertainty (e.g., MDPs). It supports two forms of uncertainty: uncertainty about the initial state, and non-deterministic action outcome. % Maybe add a def. of conformant planning and make all the mapping more formal

% Mapping model-free to conformant planning
The conformat planning problem that corresonds to a given model-free planning problem $\tuple{P,I,G, \mathcal{T}}$ has the same set of predicates ($P$) and goals ($G$). It has a single possible initial state -- $I$, i.e., there is no uncertainty over the initial state. Its actions are all the actions observed in the observed trajectories $\mathcal{T}$. The preconditions of an action $a$ are $pre_\mathcal{T}^u(a)$. The effects are non-deterministic, and are any possible combination of 
add and delete effects that is in the bounds of Equation~\ref{eq:bounds}. A solution to a conformant planning problem is a {\em conformant plan} -- a sequence of actions that is guaranteed to achieve the goal regardless of the uncertainty in the initial condition and in the nondeterministic effects of actions. Thus, a solution to the conformant planning outlined above is necessarily a sound solution to the corresponding model-free planning problem. 


% Something about the difficulty of solving 
This mapping of model-free to contingent planning is, however, not complete: if no solution was found for the contingent planning problem it still may be the case the the model-free planning has a solution. This has two possible causes: an over conservative estimate of actions' effects and an over conservative estimate of actions' preconditions. 

\subsection{Known Effects}

%Following Walsh and Littman~\shortcite{walsh2008efficientLearning}, we consider in the rest of this paper a setting in which action effects are known and the uncertainty only stems from the overconservative set of preconditions. To motivate this special case, observe that $add_\mathcal{T}^u(a)$ can only be incorrect if there is a predicate that is an add effect and it existed in all the pre-states of $a$.     Similarly, $del_\mathcal{T}^u(a)$ is incorrect only if there is a predicate that is a delete effect     and did not exists in any of the pre-states of $a$. While possible, there are many planning domains that prevent this from occuring by verifying that deleted predicates are in the pre-state and added predicates are not already in the pre-state.  
 
 Under certain conditions, $add_\mathcal{T}^u(a)$ 
 and $del_\mathcal{T}^u(a)$ are exactly the effects of $a$. These conditions are:
 (1) if predicate $f$ is a delete effect of an action $a$ then $f$ is a precondition of $a$, 
 (2) if predicate $f$ is an add effect of $a$ then $f$ must be mutually exclusive with a precondition of $a$. 
 These conditions are natural in many planning domains and Walsh and Littman~\shortcite{walsh2008efficientLearning} studies this special case in which actions' effects are known and the only uncertaitly is over actions' preconditions. 
 In the rest of this paper we focus on this special case. 
 %Walsh and Littman~\shortcite{walsh2008efficientLearning} considered a special case for learning an action model from trajectories, where action effects are known and the uncertainty only stems from the overconservative set of preconditions. In fact, under certain conditions, $add_\mathcal{T}^u(a)$  and $del_\mathcal{T}^u(a)$ are exactly the effects of $a$. 
 
 % In the rest of this paper we adopt this special case, and motivate it as follows. Observe that $add_\mathcal{T}^u(a)$ can only be incorrect if there is a predicate that is an add effect and it existed in all the pre-states of $a$.     Similarly, $del_\mathcal{T}^u(a)$ is incorrect only if there is a predicate that is a delete effect     and did not exists in any of the pre-states of $a$. While possible, there are many planning domains that prevent this from occuring by verifying that deleted predicates are in the pre-state and added predicates are not already in the pre-state.  
    
    In this case of known effects, there is a much simpler sound and incomplete approach to solving the model-free problem. Instead of compiling the problem to a conformant planning problem, we now compile it to a classical planning problem, having the same initial state, goal state, set of predicates and actions. The only difference is that the actions preconditions are the overconservative set $pre_\mathcal{T}^u(a)$. It is easy to see that a solution to this problem will also be a solution to the model-free planning problem. However, this approach is incomplete, since $a$'s precondition can be a subset of $add_\mathcal{T}^u(a)$. This can result in the planner think it cannot apply $a$ in a state that does not contain all the predicates in $add_\mathcal{T}^u(a)$, even though $a$'s actual precondition would have allowed performing $a$ in that state. A key question is how likely it is that this will occur, i.e., that usins this approach will not find a solution although one exists. We answer this question in the next section. 



%consider $add_\mathcal{T}^u(a)$ and $del_\mathcal{T}^u(a)$. 


%{\bf Overconservative effects.} A contingent plan is one that achieves the goals for all possible outcomes of the non-deterministic actions. Finding such a plan may not be possible, while finding a plan for the trueFor the add effecs, this means there was a predicate that is an add effect but since it was in all the observed trajectories our 
%An over conservative estimate of an action $a$'s preconditions means that the preconditions $a$ contain fewer predicates than $pre_\mathcal{T}^u(a)$. Thus, there are states where $a$ can be applied but the conformant planner will not consider it. 
%ing problem could not , and thus can be executed in a broader range of states. Consider the latter reason -- an over conservative estimate of the actions effects. For the add effecs, this means there was a predicate that is an add effect but since it was in all the observed trajectories our 


% If we know the effects then it is a classical planning problem, but still incomplete

% What's the probability that is complete


\section{Learning to Perform Model-Free Planning}
%[[Something about the impracticality of the solution above, and how we aim for a more general task, in which we want to maximize probability of success]]
In general, we cannot guarantee that {\em any finite number of trajectories} will suffice to obtain {\em precisely} the underlying action model. This is because, for example, if some action never appears in a trajectory, we may not know its effects; or, if an action is only used in a limited variety of states, it may be impossible to distinguish its preconditions. However, as the number of trajectories increase, what we can hope for is to learn enough of the actions accurately enough to be able to find plans for most goals in practice. This gives raise to a statistical view of the model-free planning task (Definition~\ref{def:model-free-planning}), which follow the usual statistical view of learning, along the lines of Vapnik and Chervonenkis~\shortcite{vapnik1971} and Valiant~\shortcite{valiant1984}.  %is:
%In this work, we follow the usual statistical view of learning, along the lines of Vapnik and Chervonenkis~\shortcite{vapnik1971} and Valiant~\shortcite{valiant1984}. The main task we consider is:

\begin{definition}[Model-Free Planning as a Learning Task] We suppose that there is an arbitrary, unknown (prior) probability distribution $D$ over triples of initial states, goal conditions, and trajectories for a fixed environment, in particular with a fixed set of actions $A$, predicates $P$, and preconditions and effects for the actions. We assume that these trajectories start in the corresponding initial state and end with the goal condition satisfied. In the model-free planning task, we are given a set of action triplets $(I^{(1)},G^{(1)},T^{(1)}),\ldots,(I^{(m)},G^{(m)},T^{(m)})$ drawn independendently from $D$, and we are given as input $T^{(1)},\ldots,T^{(m)}$, and a new initial state and goal $(I,G)$ for some $(I,G,T)$ drawn from $D$. We must either output a plan for $\langle P,A,I,G\rangle$ or, with probability at most $\epsilon$, assert that no plan can be found.
%In the model-free planning task, $(I^{(1)},G^{(1)},T^{(1)}),\ldots,(I^{(m)},G^{(m)},T^{(m)})$ have been drawn independendently from $D$, and we are given as input $T^{(1)},\ldots,T^{(m)}$, and a new initial state and goal $(I,G)$ for some $(I,G,T)$ drawn from $D$. We must either output a plan for $\langle P,A,I,G\rangle$ or, with probability at most $\epsilon$, assert that no plan can be found.
\end{definition}

%We note that this task is conservative in the sense that when an algorithm solving it outputs a plan, the plan is required to be correct. The algorithm may only make errors by failing to find a plan when one exists. %\section{Theoretical Guarantees and Limitations} How many trajectories do we need to guarantee that every plan can be generated?


A key question is how many trajectories do be certain enough that that for a given start and goal state, a  plan can be generated. 
\begin{theorem}
%With probability $1-\delta$, $\frac{(2ln 2)|A|}{\epsilon}(|P|+\log |A|+\log\frac{2}{\delta})$ trajectories suffice for solving the model-free planning problem.
%task of learning to plan.
Having $\frac{(2ln 2)|A|}{\epsilon}(|P|+\log |A|+\log\frac{2}{\delta})$ trajectories suffice for solving the model-free planning problem
with probability $1-\delta$. 
\end{theorem}
\begin{proof}
Let $A'$ be the set of actions that appear in trajectories sampled from $D$ with probability at least $\frac{\epsilon}{2|A|}$%, let $A''$ be the set of actions that appear with probability at least $\frac{\epsilon}{4|A|}$, 
and let $A(\mathcal{T})$ be the set of actions that appeared in {\em any} trajectory. \roni{Is $A(\mathcal{T})$ referring to any trajectory or only the observed ones?} 
We first show that the probability that any action $a\in A'$ does not appear in $A(\mathcal{T})$ is at most $\delta/2$. 
\roni{I am always confused with the term {\em any}. Do you mean that the probability that $A(\mathcal{T})\cap A'=\emptyset$ is at most $\delta/2$?}
For any action $a\in A'$, it will not be invoked in a trajectory drawn from $D$ with probability at most $1-\frac{\epsilon}{2|A|}$. 
%: indeed, fix any action $a\in A'$. We note that $a$ is not invoked in each trajectory drawn from $D$ with probability at most $1-\epsilon/2|A|$. 
%: indeed, fix any action $a\in A'$. We note that $a$ is not invoked in each trajectory drawn from $D$ with probability at most $1-\epsilon/2|A|$. 
Thus, since our trajectories are drawn independently from $D$, the probability that $a$ does not appear in $A(\mathcal{T})$ 
is $(1-\frac{\epsilon}{2|A|})^m$. Thus, using $1+x\leq e^x$ inequality, the probability that $a\notin A(\mathcal{T})$ is less than 
${e^{-\frac{\epsilon}{2|A|}}}^m$. 
Since $m\geq \frac{(2ln 2)|A|}{\epsilon}(|P|+\log |A|+\log\frac{2}{\delta})$, we have that the probability that $a\notin A(\mathcal{T})$ is less than less than $\delta/2|A|$, and hence by a union bound over $a\in A'$ (noting $|A'|\leq |A|$), $A'\subseteq A(\mathcal{T})$ with probability $1-\delta/2$ as needed.
% * <Roni Stern> 04:11:53 22 Nov 2016 UTC+0200:
% I could not follow this hop. Can you explain?

%$\frac{(2ln 2)|A|}{\epsilon}(|P|+\log |A|+\log\frac{2}{\delta})$


%is $(1-\epsilon/2|A|)^m$ where $m > \frac{(2\ln 2)|A|}{\epsilon}\log\frac{2|A|}{\delta}$.
%is $(1-\frac{\epsilon}{2|A|})^m$ where $m > \frac{(2\ln 2)|A|}{\epsilon}\log\frac{2|A|}{\delta}$. Thus, using $1+x\leq e^x$, the probability that $a\notin A(\mathcal{T})$ is less than $\delta/2|A|$, and hence by a union bound over $a\in A'$ (noting $|A'|\leq |A|$), $A'\subseteq A(\mathcal{T})$ with probability $1-\delta/2$ as needed.
We will call an assignment of preconditions {\em unsafe} (for $A(\mathcal{T})$) if at least one term in at least one of the preconditions for at least one of the actions is missing. We will call an assignment of preconditions {\em inadequate} if, with probability at least $\epsilon/2$ over trajectories $T$ sampled from $D$, there is an action $a\in A'$ and a state where $a$ is invoked in $T$ such that the precondition we assign to $a$ is not satisfied.  Observe that if we can find an assignment of preconditions to actions that is neither unsafe for $A(\mathcal{T})$ nor inadequate for $A'$, then for $(I,G)$ sampled from $D$, with probability $1-\epsilon/2$ there is a plan for $(I,G)$ that only uses actions in $A'$ since in particular, the corresponding, unknown trajectory uses an action outside $A'$ with probability at most $|A|\cdot\frac{\epsilon}{2|A|}=\epsilon/2$, and likewise since the preconditions are not inadequate for $A'$, with probability $1-\epsilon/2$ the specified preconditions are satisfied on all of the states in which the corresponding action is used in the unknown trajectory; thus, with probability $1-\epsilon$, the plan in the trajectory is a solution to the planning task in which the actions are restricted to $A'\subseteq A(\mathcal{T})$ (where containment holds with probability $1-\delta/2$, as established above). Moreover, since the assignment of preconditions to actions is not unsafe, if we only use actions in $A(\mathcal{T})$, whenever our precondition for $a\in A(\mathcal{T})$ is satisfied for a state in our trajectory, the real precondition is also satisified. 

It thus suffices to argue that the algorithm finds an assignment of preconditions to actions that is neither unsafe for $A(\mathcal{T})$ nor inadequate for $A'$. We first observe that there are $2^{|A||P|}$ candidate preconditions over $|P|$ for our $|A|$ actions. Now, the subset $BAD$ of these assignments of preconditions that are inadequate for at least one action $a\in A'$ in particular has size at most $2^{|A||P|}$. We note that any particular inadequate assignment cannot be produced as output by the algorithm if we draw a trajectory in which for one of the actions in $a\in A'$ the precondition assigned to $a$ by that inadequate assignment is not satisfied on one of the states in the trajectory in which $a$ is invoked. Note that by the definition of inadequacy, for each example trajectory drawn, such a state appears with probability at least $\epsilon/2$. Therefore, the probability that no state appears in our $m$ independently drawn trajectories is at most $(1-\epsilon/2)^m$. Concretely, since $m\geq\frac{(2ln 2)|A|}{\epsilon}(|P|+\log\frac{2|A|}{\delta})>\frac{2\ln 2}{\epsilon}(|A||P|+\log\frac{2}{\delta})$, this is at most $\delta/2^{|A||P|+1}$. Thus, by a union bound over this set of inadequate assignments $BAD$, the probability that any inadequate assignment of preconditions could be output is at most $\delta/2$.


%We next observe that by a Chernoff bound, the probability that an action $a\in A'$ does not appear in at least an $\epsilon/3|A|$-fraction of the $m$ example trajectories is at most $\exp(-\frac{1}{2}36m\frac{\epsilon}{2|A|})<\delta/2|A|$, and that the probability that an action $a\notin A''$ appears in at least a $\epsilon/3|A|$-fraction of the example trajectories is at most $\exp(-\frac{1}{3}144m\frac{\epsilon}{4|A|})<\delta/2|A|$. Thus by a union bound over the (at most $|A|$) actions in $A'$ or outside $A''$, with probability $1-\delta/2$, all of the actions in $A'$ appear in sufficiently many of the trajectories that our estimates are adequate and no action outside $A''$ appears so often.

We finally note that the algorithm cannot produce unsafe preconditions for $A(\mathcal{T})$: indeed, observe that the preconditions are satisfied for every action invoked in every trajectory obtained by the algorithm. Since the algorithm includes all of the literals that are satisfied on all of the states in which that action was invoked, in particular it includes all of the literals that actually appear in the precondition. Thus, with probability $1-\delta$, the algorithm indeed produces an assignment of preconditions that is neither unsafe for $A(\mathcal{T})$ nor inadequate for $A'$, as needed.
\end{proof}


\section{Related Work}
Mour{\~{a}}o et al.~\shortcite{mourao2012learning} addressed the problem of learning a STRIPS actions given a set of observed trajectories. In their setting, the objserved trajectories consists cases in which the agent tried to perform an action an failed, while in our case the observations are only successful trajectories. Also, we assume full observability while they considered partial and noisy observations of the states in the trajectories. Their approach was to use machine learning methods to predict the outcome of an action and to predict whether it is applicable. Consequently, the plan generated by using this model can fail. By contrast, we aim for a plan that is guaranteed to work. Another similarity of this paper to the paper Mour{\~{a}}o et al.~\shortcite{mourao2012learning} 

Also related is the work of Konidarid et al.~\shortcite{konidaris2014constructing} showed how to learn a STRIPS model that provides a useful high-level plan for a continuous world. 


The mentioned above work of Wang~\shortcite{wang1995learning,wang1994learning} is, of course, very related to our work. 
They proposed a process for learning a STRIPS model that includes learning an action model. Unlike our work, they discussed how to refine that action model by interacting with the world, including learning from failed 
execution of actions.  We require that the plan generated by the learned action {\em must} work. This is suitable for setting where plan failure is unacceptable, e.g., where failure corresponds to physical harm to the acting agent.  


Levine and DeJong~\shortcite{levine2006explanation} proposed how to learn control operators 
from background knowledge and experimentations. A similar approach is taken in many re-inforcement learning algorithms. Again, our task is different, in that we do not allow experimentation and aim for a plan that is guaranteed to be sound.

Jim{\'e}nez et al.~\shortcite{jimenez2013integrating} also studied how to integrate learning of planning operators and planning. Their approach included an option to experiment with planning rules that are probably correct, while we aim for a provably sound plan. 

Walsh and Littman~\shortcite{walsh2008efficientLearning} also discuss the problem of learning STRIPS operators from observed trajectories. They too interleve planning and execution, where they learn an action model, find a plan for a given planning problem, and update that action model by the success or failure of the generated plan. Unlike other work, they also provide a theoretical bounds on the sample complexity -- the number of interactions needed in order to guarantee that the resulting planner is sound and complete. By contrast, we do not assume any planning and execution loop, and we aim for a planning algorithm that is guaranteed to be sound, at the cost of completeness. 
This affects their approach to learning: they attempted to follow an optimistic assumption about the preconditions and effects of the learned actions, in an effort to identify inaccuracies in their action model. By contrast, we are forced to take a pessimistic approach, as we aim for a successful execution of the plan rather than information gathering to improve the action model. 


\section{Conclusions}
This paper deals with a planning problem in which the planner agent does not know the agent's actions. Instead of an action model, the planner is given a set of observed trajectories of succesfully executed plans. 
In this setting we introduce the {\em model-free planning} problem, in which the task is to find a plan that is guaranteed to reach the goal, i.e., there is no tolerance for execution failure. This type of problem is important in cases where failure is costly or in cases where the agent has no capability to replan during execution. 
We show how to learn from the given set of trajectories 
information about the agents actions, including bounds on the preconditions and effects of actions used in the given trajectories. Then, we propose a solution to the model-free problem that is based on translation to conformant planning. The solution is sound but is not complete, as it may fail to find a solution if one exists. 
Then, we prove that under some assumptions the likelihood of finding a solution with this approach grows linearly with the number of predicates and super-linear with the number of actions. 
Future work on model-free planning include studying how to relax the simplifying assumptions made about the action's effects, how to consider richer underlying planning models including parametric actions, conditional effects, stochastic action outcomes, and partial observability.  



\newpage

\bibliography{library}
\bibliographystyle{aaai}

\end{document}

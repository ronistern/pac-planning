%% It is just an empty TeX file.
%% Write your code here.


\section{Learning a Conservative Model}
We follow Wang's~\shortcite{wang1994learning,wang1995learning} approach for learning a STRIPS model from observation and interactions. For every action $a$ that was used in the observed trajectories ($\mathcal{T}$) a triplet of the form 
$\langle P, a, D\rangle$, where $P$ is the state before $a$ was applied and $D$ is the {\em delta state}, which represents the difference between $P$ and the state observed after applying $a$ to $P$. Formally, a delta-state is a pair $\langle add, del \rangle$, where $\mathtt{add}$ and $\mathtt{DEL}$ are conjunction of literals that describe the facts added to and deleted from $P$ in the state observed after $a$ was performed. %are both conjuncts of ground literals describing the literals that are added to and deleted from the pre-state after the operator is applied. The delta-state is the dierence between the post-state and pre-state

%$\langle s, a, s'\rangle$ is generated for every occurence of $a$ in $\mathcal{T}, such that $s$ is the state before applying $a$ and $s'$ is the state afterwards. 


From the set of observed trajectories, we extract triplets of the form $\langle s, a, s'\rangle$

They maintain for each action $a$ two sets of possible preconditions: the set of most specific preconditions ($S(a)$) and the set of most general ($G(a)$) preconditions. 

Under some simplifying assumptions, which we outline below, the set of most specific preconditions $S(a)$ consists a sufficient condition for executing $a$. Under this condition, any plan that uses $S(a)$ as the set of preconditions to $a$ is guaranteed to find the goal, resulting in a sound yet incomplete solution. 

The interesting questions are: how to compute $Sa(a)$, what are the assumptions that are required for $S(a)$ to indeed be sufficient precdonios, and how many trajectories are needed to provide some guarantee of completeness. We address these questions below. 


%(no negative preconditions, no noise, full observability), it holds that the real preconditions of $a$ are a superset of $G(a)$ and a subset of $S(a)$. Thus, we can solve the model-free planning problem by generating a plan that assumes the preconditions in $S(a)$ are the true preconditions. Since they are sufficient condition, we know that the plan will succeed. 


%, $S(a)$ and $G(a)$, where $S(a)$ is a set of facts that are sufficient preconditions and $G(a)$ is a set of facts that are required preconditions. They refer to these sets as the set of most specific ($S(a)$) and the set of most general ($G(a)$) preconditions. Under some simplifying assumptions (no negative preconditions, no noise, full observability), it holds that the real preconditions of $a$ are a superset of $G(a)$ and a subset of $S(a)$. Thus, we can solve the model-free planning problem by generating a plan that assumes the preconditions in $S(a)$ are the true preconditions. Since they are sufficient condition, we know that the plan will succeed. 


We propose a conservative approach to solve the model-free planning problem
that first learns a partial model from the given trajectories and then uses it to generate a plan that is guaranteed to reach the goal. For a real, unknown, planning problem $\Pi^*=\langle P,A,I,G\rangle$ we will analyze the observed trajectories to learn the following planning problem $\Pi_L=\langle P,A_L,I,G\rangle$, where $A_L$ is the learned actions. 
Ideal
We aim that $A_L$ will 


From the set of observed trajectories ($\mathcal{T}$), we extract the set of actions that were part of the observed trajectories, 
denoted by $A(\mathcal{T})$. Formally,
\[ A(\mathcal{T})=\{a | \exists T\in\mathcal{T} \text{~such that~} a\in T\} \]
Next, we generate for action $a\in A(\mathcal{T})$ a set of preconditions and effects, by considering the states that were before and after $a$ in the trajectories where it appears. 
Let $T(a)$ be the set of trajectories in $\mathcal{T}$ in which $a$ appeared, i.e., 
\[ T(a)=\{T | T\in \mathcal{T} \text{~and~} a\in T\} \]


We define the preconditions and effects of $a$, denoted $pre(a)$ and $eff(a)$, respectively, as follow:
\begin{itemize}
    \item {\bf Preconditions.} The intersection of predicates that were true in all the state that preceded $a$ in the given trajectories $\mathcal{T}$. 
    \item {\bf Effects.} The difference, predicates, between states immediately before $a$ and states immediately after $a$. 
\end{itemize}

The effects are, of course, the real effects of $a$. However, the preconditions are a superset of the preconditions of $a$. Thus, every plan generated with these actions is a valid plan in the real model, but we might not find a plan we can generate even though such exists. 

MAYBE TALK ABOUT COSTS (E.G., ALL POSSIBLE SOLUTIONS TO A LINEAR EQUATION, IF WE HAVE THE COSTS OF THE TRAJECTORIES)
% hich learns from the observed trajectories a partial model that is ``safe'' to use in planning.  This results in a model that can generate a subset of the plans that the original, unknown, model can generate, but the generated plan are guaranteed to be applicable. 


%\section{Model-Free Planning as Conformant Planning}
%HERE WE'LL TALK ABOUT HOW THE ABOVE COMPILES TO CONFORMANT PLANNING,SO WE CAN JUST RUN A SUITABLE PLANNING. ALSO, SOME OF THESE PLANNERS JUST COMPILE TO CLASSICAL PLANNERS, SO THAT'S EVEN BETTER. 






























as 

The preconditions and effects of actions are defined to ensure that 
if an action is applicable in $F(\Pi_\mathcal{T})$ then it is also be applicable in the underlying planning problem $\Pi$ and it will have exactly the same effects in $F(\Pi_\mathcal{T})$ and in $\Pi$. To this end, we define in $F(\Pi_\mathcal{T})$ the preconditions of an action $a$ to be the ``upper bound'' estimate given in Equation~\ref{eq:pre} -- $\bigcap_{\tuple{s, a, s'}\in \mathcal{T}(a)} s$ -- 
and define the effects of $a$ to be the ''lower bound'' estimate given in 
Equation~\ref{eq:eff} -- $\bigcup_{\tuple{s,a,s'}\in\mathcal{T}(a)} s'\setminus s$. 





%%%% TO HERE %%%


. %=\tuple{\mathcal{X}, \mathcal{O}',s_\mathcal{I}, s_\mathcal{G}}$. 
Observe that $F(\Pi_\mathcal{T})$


$\Pi_\mathcal{T}$ 


over actions' preconditions and effects, we propose to solve the model-free planning problem by compiling it to a classical \SAS{} problem. 
Let $\Pi_\mathcal{T}$ be the 



a {\em conservative} approach to solve the model-free planning problem that is based on compiling it to a fully observable non-deterministic (FOND) planning problem~\cite{daniele1999strong,muise2012improved}. 
A FOND planning problem is defined like a classical planning problem, except that the effect of an action $a$
is not a set of values assignments but a set of sets of value assignments, where each set of value assignments represents a possible outcome of applying $a$.  


We transform a given conformant model-free planning problem $\Pi_\mathcal{T}$ to a FOND problem $F(\Pi_\mathcal{T})$ as follows. $F(\Pi_\mathcal{T})$ has the same set of state variables ($\mathcal{X}$), start state ($s_\mathcal{I}$), and goal ($s_\mathcal{G}$) as $\Pi_\mathcal{T}$. The actions of $F(\Pi_\mathcal{T})$ is are all the actions seen in an observed trajectory. We denote this set of actions by $A(\mathcal{T})$. 
The preconditions and effects of actions are defined to ensure that 
if an action is applicable in $F(\Pi_\mathcal{T})$ then it will also be applicable in the underlying planning problem $\Pi$ and it will have exactly the same effects in $F(\Pi_\mathcal{T})$ as it had in $\Pi$. To this end, we define in $F(\Pi_\mathcal{T})$ the preconditions of an action $a$ to be the ``upper bound'' estimate given in Equation~\ref{eq:pre} -- $\bigcap_{\tuple{s, a, s'}\in \mathcal{T}(a)} s$ -- 
and define the effects of $a$ to be the ''lower bound'' estimate given in 
Equation~\ref{eq:eff} -- $\bigcup_{\tuple{s,a,s'}\in\mathcal{T}(a)} s'\setminus s$. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Fully Observable Non-Deterministic Planning}
Given the bounds in Equation~\ref{eq:pre} and~\ref{eq:eff} over actions' preconditions and effects, we propose a {\em conservative} approach to solve the model-free planning problem that is based on compiling it to a fully observable non-deterministic (FOND) planning problem~\cite{daniele1999strong,muise2012improved}. 
A FOND planning problem is defined like a classical planning problem, except that the effect of an action $a$
is not a set of values assignments but a set of sets of value assignments, where each set of value assignments represents a possible outcome of applying $a$.  


We transform a given conformant model-free planning problem $\Pi_\mathcal{T}$ to a FOND problem $F(\Pi_\mathcal{T})$ as follows. $F(\Pi_\mathcal{T})$ has the same set of state variables ($\mathcal{X}$), start state ($s_\mathcal{I}$), and goal ($s_\mathcal{G}$) as $\Pi_\mathcal{T}$. The actions of $F(\Pi_\mathcal{T})$ is are all the actions seen in an observed trajectory. We denote this set of actions by $A(\mathcal{T})$. 
The preconditions and effects of actions are defined to ensure that 
if an action is applicable in $F(\Pi_\mathcal{T})$ then it will also be applicable in the underlying planning problem $\Pi$ and it will have exactly the same effects in $F(\Pi_\mathcal{T})$ as it had in $\Pi$. To this end, we define in $F(\Pi_\mathcal{T})$ the preconditions of an action $a$ to be the ``upper bound'' estimate given in Equation~\ref{eq:pre} -- $\bigcap_{\tuple{s, a, s'}\in \mathcal{T}(a)} s$ -- 
and define the effects of $a$ to be the ''lower bound'' estimate given in 
Equation~\ref{eq:eff} -- $\bigcup_{\tuple{s,a,s'}\in\mathcal{T}(a)} s'\setminus s$. 

TODO: Prove that this is really a save definition of the operators

\roni{Up to  here}
%%%%%%%% UP TO HERE %%%%%%%%%%55


We use the bounds in Equation~\ref{eq:pre} and~\ref{eq:eff} to 
transform a given conformant model-free planning problem $\Pi_\mathcal{T}$ 
to a fully observable non-deterministic (FOND) planning problem~\cite{daniele1999strong,muise2012improved}, 
as follows. 
A FOND planning problem is defined like a classical planning problem, except that the effects of an action $a$
is not a set of values assignments but a set of sets of value assignments, where each set of value assignments represents a possible outcome of applying $a$.  


denoted $F(\Pi\mathcal{T})$

solve a given conformant model-free planning problem $\Pi_\mathcal{T}$ by transforming it to a fully observable non-deterministic (FOND) planning problem~\cite{daniele1999strong,muise2012improved} $F(\Pi_\mathcal{T})$ such that every to $F(\Pi_\mathcal{T})$ 
is a possible solution to the underlying planning problem $\Pi$. Then, we 


transform  





to a fully observable non-deterministic (FOND) planning problem~\cite{daniele1999strong,muise2012improved}

, 
and show how solving this FOND problem is related to solutions of the given conformant model-free planning problem.  
as follows. 



We denote by $pre_\mathcal{T}^u(a)$ the set of ``upper bound'' on the preconditions of $a$ 
given in Equation~\ref{eq:pre} (i.e., $pre_\mathcal{T}^u(a)=\bigcap_{\tuple{s, a, s'}\in \mathcal{T}(a)} s$). 
Given these bounds on the facts in the pre-conditions and effects, we propose a {\em conservative} approach to solve the model-free planning problem. In this approach, we create a planning problem that has the same predicates, initial state, and goal condition as the conformant model-free planning problem. Its actions are all the actions observed in $\mathcal{T}$. The preconditions of an action are the ``upper'' bound preconditions $pre_\mathcal{T}^u(\cdot)$. The effects are non-deterministic, and consist of any possible combination of add and delete effects in the bounds. 

$\tuple{\mathcal{X},s_\mathcal{I}, s_\mathcal{G}, \mathcal{T}}$

given $\mathcal{T}$ 




. Similarly, we denote $add_\mathcal{T}^u(a)$, $add_\mathcal{T}^l(a)$, $del_\mathcal{T}^u(a)$, and $del_\mathcal{T}^l(a)$, as the upper and lower bounds of the add and delete effects. 



For convenience, we denote by $pre_\mathcal{T}^u(a)$ the set of ``upper bound'' on the preconditions of $a$ given $\mathcal{T}$. Similarly, we denote $add_\mathcal{T}^u(a)$, $add_\mathcal{T}^l(a)$, $del_\mathcal{T}^u(a)$, and $del_\mathcal{T}^l(a)$, as the upper and lower bounds of the add and delete effects. 


Given these bounds on the facts in the pre-conditions and effects, we propose a {\em conservative} approach to solve the model-free planning problem. In this approach, we create a planning problem that has the same predicates, initial state, and goal condition as the conformant model-free planning problem. Its actions are all the actions observed in $\mathcal{T}$. The preconditions of an action are the ``upper'' bound preconditions $pre_\mathcal{T}^u(\cdot)$. The effects are non-deterministic, and consist of any possible combination of add and delete effects in the bounds. 

\subsection{Conformant Planning}
There are several effective fully observable non-deterministic (FOND) planners~\cite{cimatti2003weak,kissmann2009solving}. We aim, however, to find a  solution to our non-deterministic planning problem that is guaranteed to be applicable. This is known as requiring a ``strong plan'' in the FOND literature. A particularly related type of FOND is {\em conformant planning}. Conformant planning can be seen as a middle ground between classical planning and full-blown planning under uncertainty (e.g., MDPs). It supports two forms of uncertainty -- uncertainty about the initial state, and non-deterministic action outcomes -- and aims to find a plan that is guaranteed to reach the goal.

% Maybe add a def. of conformant planning and make all the mapping more formal

% Now we can have a conformant planning problem and solve it



%Given these bound on the facts in the pre-conditions and effects, we can devise a {\em conformant planning} problem that, if solveable, will guarantee a solution to the model-free planning problem. Conformant planning can be seen as a middle ground between classical planning and full-blown planning under uncertainty (e.g., MDPs). It supports two forms of uncertainty: uncertainty about the initial state, and non-deterministic action outcome. % Maybe add a def. of conformant planning and make all the mapping more formal

% Mapping model-free to conformant planning
The conformant planning problem that corresponds to a given conformant model-free planning problem $\tuple{P,I,G, \mathcal{T}}$ has the same set of predicates ($P$) and goals ($G$). It has a single possible initial state ($I$), i.e., there is no uncertainty over the initial state. Its actions are all the actions observed in the observed trajectories $\mathcal{T}$. The preconditions of an action $a$ are $pre_\mathcal{T}^u(a)$. The effects are non-deterministic, and consist of any possible combination of 
add and delete effects that is within the bounds of Equation~\ref{eq:bounds}. A solution to a conformant planning problem is a {\em conformant plan} -- a sequence of actions that is guaranteed to achieve the goal regardless of the uncertainty in the initial condition and in the nondeterministic effects of actions. Thus, a solution to the conformant planning outlined above is necessarily a sound solution to the corresponding model-free planning problem. 


% Something about the difficulty of solving 
This approach of mapping of conformant model-free planning to regular  conformant planning is sound but not complete: if no solution was found for the conformant planning problem it still may be the case the conformant model-free planning has a solution. 

\subsection{Known Effects}

%Following Walsh and Littman~\shortcite{walsh2008efficientLearning}, we consider in the rest of this paper a setting in which action effects are known and the uncertainty only stems from the overconservative set of preconditions. To motivate this special case, observe that $add_\mathcal{T}^u(a)$ can only be incorrect if there is a predicate that is an add effect and it existed in all the pre-states of $a$.     Similarly, $del_\mathcal{T}^u(a)$ is incorrect only if there is a predicate that is a delete effect     and did not exists in any of the pre-states of $a$. While possible, there are many planning domains that prevent this from occuring by verifying that deleted predicates are in the pre-state and added predicates are not already in the pre-state.  
The above approach for solving the conformant model-free planning problem is incomplete because of two causes: an over-conservative estimate of actions' effects or an over-conservative estimate of actions' preconditions. Under certain conditions, the learned ``upper'' bound on the effects ($add_\mathcal{T}^u(a)$ and $del_\mathcal{T}^u(a)$) is exactly the effects of $a$. These conditions are:
(1) if a predicate $f$ is a delete effect of an action $a$ then $f$ is a precondition of $a$, 
(2) if a predicate $f$ is an add effect of $a$ then $f$ must be mutually exclusive with a precondition of $a$. 
These conditions are natural in many planning domains and Walsh and Littman~\shortcite{walsh2008efficientLearning} studies this special case in which actions' effects are known and the only uncertaitly is over actions' preconditions. 
%Walsh and Littman~\shortcite{walsh2008efficientLearning} considered a special case for learning an action model from trajectories, where action effects are known and the uncertainty only stems from the overconservative set of preconditions. In fact, under certain conditions, $add_\mathcal{T}^u(a)$  and $del_\mathcal{T}^u(a)$ are exactly the effects of $a$. 

% In the rest of this paper we adopt this special case, and motivate it as follows. Observe that $add_\mathcal{T}^u(a)$ can only be incorrect if there is a predicate that is an add effect and it existed in all the pre-states of $a$.     Similarly, $del_\mathcal{T}^u(a)$ is incorrect only if there is a predicate that is a delete effect     and did not exists in any of the pre-states of $a$. While possible, there are many planning domains that prevent this from occuring by verifying that deleted predicates are in the pre-state and added predicates are not already in the pre-state.  

In this case of known effects, there is a much simpler sound and incomplete approach to solving the model-free problem. Instead of compiling the problem to a conformant planning problem, we now compile it to a classical planning problem, having the same initial state, goal state, set of predicates, and actions. The only difference is that the actions' preconditions are the over-conservative sets $pre_\mathcal{T}^u(a)$. It is easy to see that a solution to this problem will also be a solution to the model-free planning problem. However, this approach is incomplete, since $a$'s precondition can be a subset of $pre_\mathcal{T}^u(a)$. This can result in the planner thinking that it cannot apply $a$ in a state that does not contain all the predicates in $pre_\mathcal{T}^u(a)$, even though $a$'s actual precondition would have allowed it to perform $a$ in that state. 


In the rest of this paper we focus on the case of known effects and address the following key question: how likely it is that the above approach is incomplete, i.e., will not find a solution to the conformant model-free planning problem even though one exists. We answer this question in the next section. 



%consider $add_\mathcal{T}^u(a)$ and $del_\mathcal{T}^u(a)$. 


%{\bf Overconservative effects.} A contingent plan is one that achieves the goals for all possible outcomes of the non-deterministic actions. Finding such a plan may not be possible, while finding a plan for the trueFor the add effecs, this means there was a predicate that is an add effect but since it was in all the observed trajectories our 
%An over conservative estimate of an action $a$'s preconditions means that the preconditions $a$ contain fewer predicates than $pre_\mathcal{T}^u(a)$. Thus, there are states where $a$ can be applied but the conformant planner will not consider it. 
%ing problem could not , and thus can be executed in a broader range of states. Consider the latter reason -- an over conservative estimate of the actions effects. For the add effecs, this means there was a predicate that is an add effect but since it was in all the observed trajectories our 


% If we know the effects then it is a classical planning problem, but still incomplete

% What's the probability that is complete




\section{Beyond the Known Effects Case}
The above analysis holds for the ``Known Effects'' case, i.e., where observing an action in a trajectory reveals the effects of that action. As discussed above this case is true when both of the following conditions hold:
(1) if a fact $f$ is a delete effect of an action $a$ then 
$f$ must also be a pre-condition of $a$, and
(2) if a fact $f$ is an add effect of an action $a$ then 
$f$ must not be true in the state before applying $a$. 
These conditions are reasonable in some domains but less so in others. In this section we show how to compile a domain that does not have these properties to a domain where these properties exist.
Then, we use this compilation to extend Theorem~\ref{the:pac-conformant} to the general case (beyond the known effects case). 


% The compilation
An action for which the above two conditions hold is referred to as a {\em regular} action. The ``Known Effects'' case is when all actions in a planning domain are regular. 
Let $\mathcal{D}=(P,A)$ be a planning domain that contains actions that are not regular. We show next how to create a planning domain $\mathcal{D}_R$ that is regular and is equivalent to $\mathcal{D}$. 

The set of actions in $\mathcal{D}_R$ contain all the regular actions in $\mathcal{D}$. Addititional actions are added to represent the functionality of the non-regular actions in $\mathcal{D}$, as follows. Let $a$ be an action in $\mathcal{D}$ that is not regular. There are two (not mutually exclusive) reasons why $a$ is not regular. Either it has one or more facts that are delete effects but are not in $pre(a)$, or that it has one or more facts that are add effects and may exists in a state in which $pre(a)$ holds. 

Consider first the former option, where there's a fact $f$ that is a delete effect of $a$ and not in $pre(a)$. For such a case, we add to $\mathcal{D}_R$ two actions: $a_{f+}$ and $a_{f-}$, defined as follows:
\begin{itemize}
	\item $a_{f+}$ is equivalent to $a$, except that it has $f$ as a precondition. 
	\item $a_{f-}$ is equivalent to $a$, except that it has $\neg f$ as a precondition and does not have $f$ as a delete effect.
\end{itemize}

Similarly, for the case where $f$ is an add effect of $a$ but can be true in a state where $pre(a)$ are true, we create two actions $a_{f+}$ and $a_{f-}$, defined as follows. 

\begin{itemize}
	\item $a_{f+}$ is equivalent to $a$, except that it has $f$ as a precondition and does not have $f$ as an add effect.
	\item $a_{f-}$ is equivalent to $a$, except that it has $\neg f$ as a precondition.
\end{itemize}

It is easy to see that $\mathcal{D}_R$ is equivalent to $\mathcal{D}$ in terms of the set of plans that can be generated in it. However, this compilation is costly, since the number of actions in this regular domain can be much larger than that of $\mathcal{D}$. Specifically, if an action $a$ has two facts that cause $a$ to be non-regular, then we will have four actions to represent $a$ in $\mathcal{D}_R$. In general if we have $n$ facts that cause $a$ to be non-regular this would result in $2^n$ actions in $\mathcal{D}_R$ to represent them.

\MEMO{Roni:I'm not sure how this goes into formula the theorem}



\subsection{The Learning Algorithm}
The observed trajectories may be from non-regular domains. 

We will learn different copies of the action $a$. 
Whenever we observe that $a$ has a different set of effects
than the set observed so far, 


Issues with the above solution: 

(1) we now add the option to have $\neg f$ as a pre-condition. This makes some of the above problematic. I think we can hack it by adding some new facts. 

(2) does it make the complexity of learning much higher

(3) 

\subsection{Post Meeting insights}

1. Approach \#1 to handle effects: include negative facts as pre-conditions
This means that every fact that does not exists in a state 
is initially assumed to be a negative pre-conditions. 
This way, we can safely learn effects, because we know that 
the facts in the add effects and the facts in the delete Effects
have  really been observed as added/deleted [[need to prove this but sounds Ok]]

2. Approach \#2 to handle effects: not include negative facts as pre-conditions
and do the action splitting approach from above. 


3. The need for a distribution over the trajectories is needed
to define probabilities over action selections. 




\section{Topics to mention or discuss}
In the \SAS{} formalism we considered, the effects and preconditions 
are just sets of value assignments $x_i=v$. However, more involved 
action models can assign a value to one state variable 
that is a function of the values of other state variables, 
e.g., $x_i=s(x_j) xor s(x_k)$. This is not covered in this work
but maybe we can learn simple assignments in these cases and bound how bad this will be?
Or, alternatively, search in the space of possible formulaes for a compact representation
that matches the observations and say something about this also.  






The action model we are learning ($\mathcal{O}_{F(\Pi_\mathcal{T})}$) is fundamentally an assignment of preconditions and effects to actions, where preconditions and effects are assignments of values to state variables (i.e., of the form $x_i=v$). We call such an assignment {\em unsafe} if 
there is an action $a$ that is missing a precondition, 
or if there is an action that has an extra effect, where ``missing'' and ``extra'' 
is with respect to the action model of the underlying planning problem ($\Pi$). 
We will call an assignment of action model {\em inadequate} if, with probability at least $\epsilon/2$ over trajectories $T$ sampled from $D$, 
there is an action $a\in A_\epsilon$ and a state where $a$ is invoked in $T$ such that the precondition we assign to $a$ in our conservative model-free planner is not satisfied. 




has a precondition $x_i=v$ according to $\mathcal{O}_\Pi$ 
but 

of 
if its contains an action 
whose preconditions is not athat is missing a precondition

, or an action that has an effect (where ``missing'' and ``incorrect'' is with respect to the action model of the underlying planning problem $\Pi$). 


that has  exists an action 

We will call an action model {\em inadequate} if, with probability at least $\epsilon/2$ over trajectories $T$ sampled from $D$, 
there is an action $a\in A_\epsilon$ and a state where $a$ is invoked in $T$ such that the precondition we assign to $a$ in our conservative model-free planner is not satisfied. 



We call an action model {\em unsafe} 
if a planner that uses it may create plans that are not applicable in the underlying planning problem $\Pi$. An action model is {\em inadequate} if with non-negligible probability 
a planner that uses it will not find a plan even if such exists. 
Put formally, an action model is unsafe if there is an action 
that is missing a precondition or an action  that has an incorrect effect (where ``missing'' and ``incorrect'' is with respect to the action model of the underlying planning problem $\Pi$). 
An action is inadequate 

has a precondition 
$\mathcal{O}$ is unsafe w.r.t an action model $\mathcal{O}_\Pi$
if either $\exists a\in A ~ \pre_(a)$

with non-negligible probability

a planner that uses it may

using it leads to plansa state that is not reachable 


it can lead 
if there exists an action $a$ such that 
there is a
according to $\mathcal{O}$ 
We will call an assignment of preconditions {\em inadequate} if, with probability at least $\epsilon/2$ over trajectories $T$ sampled from $D$, 
there is an action $a\in A_\epsilon$ and a state where $a$ is invoked in $T$ such that the precondition we assign to $a$ in our conservative model-free planner is not satisfied. 

(for $A(\mathcal{T})$) 
if at least one term  in at least one of the preconditions 
for at least one of the actions is missing. 
We will call an assignment of preconditions {\em inadequate} if, with probability at least $\epsilon/2$ over trajectories $T$ sampled from $D$, 
there is an action $a\in A_\epsilon$ and a state where $a$ is invoked in $T$ such that the precondition we assign to $a$ in our conservative model-free planner is not satisfied. 


\roni{If space is an issue then we can ommit the unsafe part, as it is discussed before}



\begin{lemma}
	If $A_\epsilon\subseteq A(\mathcal{T})$ and the learned action model is adequate, then for a start-goal pair $(s_\mathcal{I},s_\mathcal{G})$ sampled from $D$, with probability $1-\epsilon$ there is a plan for $(s_\mathcal{I},s_\mathcal{G})$ that only uses actions in $A(\mathcal{T})$ and that plan will be found by our conservative model-free planner.
	%, that in particular will be found by our conservative model-free planner.
	\label{lem:plan-existance}
\end{lemma}
\begin{proof} 
	Let $T$ be the (unknown) trajectory sampled for $(s_\mathcal{I},s_\mathcal{G})$. 
	%We prove Lemma~\ref{lem:plan-existance} by showing that  	with probability at most $\epsilon/2$ the trajectory $T$ contains an action that was not observed in a trajectory (i.e., we did not learn about its existence)  	and with probability at most $\epsilon/2$ the actions in $T$ cannot be applied due to our conservative precondition learning. 
	The probability that $T$ uses an action that is not in 
	$A_\epsilon$ is at most $|A|\cdot\frac{\epsilon}{2|A|}=\epsilon/2$. 
	Conversely, with probability higher than $1-\epsilon/2$, there exists a trajectory from   $s_\mathcal{I}$ to $s_\mathcal{G}$ that only uses actions in $A_\epsilon/2$. 
	
	Since the action model is adequate then with probability $1-\epsilon/2$ the learned preconditions are satisfied on all of the states in $T$. Thus, by a union bound, we find that with probability $1-\epsilon$, the plan in the trajectory is a solution to the planning task in which the actions are restricted to $A_\epsilon\subseteq A(\mathcal{T})$.
	Moreover, since the assignment of preconditions to actions is safe (Theorem~\ref{the:safeness}), if we only use actions in $A(\mathcal{T})$, whenever our precondition for $a\in A(\mathcal{T})$ is satisfied for a state in our trajectory, the real precondition is also satisfied.
	
	
	
	
	
	First, observe that for an assignment of preconditions to actions that is adequate for $A_\epsilon$, then for $(s_\mathcal{I},s_\mathcal{G})$ taken from $(s_\mathcal{I},s_\mathcal{G},T)$ sampled from $D$, with probability $1-\epsilon/2$ there is a plan for $(s_\mathcal{I},s_\mathcal{G})$ that only uses actions in $A_\epsilon$: in particular for the full triple $(s_\mathcal{I},s_\mathcal{G},T)$, the (unknown) $T$ uses an action outside $A_\epsilon$ with probability at most $|A|\cdot\frac{\epsilon}{2|A|}=\epsilon/2$.
	Likewise since the preconditions are adequate for $A_\epsilon$, with probability $1-\epsilon/2$ the specified preconditions are satisfied on all of the states in which the corresponding action is used in the unknown trajectory; thus, by a union bound, we find that with probability $1-\epsilon$, the plan in the trajectory is a solution to the planning task in which the actions are restricted to $A_\epsilon\subseteq A(\mathcal{T})$.
	Moreover, since the assignment of preconditions to actions is safe (Theorem~\ref{the:safeness}), if we only use actions in $A(\mathcal{T})$, whenever our precondition for $a\in A(\mathcal{T})$ is satisfied for a state in our trajectory, the real precondition is also satisfied.
	
	
	
	
	
	
	
	First, observe that for an assignment of preconditions to actions that is adequate for $A_\epsilon$, then for $(s_\mathcal{I},s_\mathcal{G})$ taken from $(s_\mathcal{I},s_\mathcal{G},T)$ sampled from $D$, with probability $1-\epsilon/2$ there is a plan for $(s_\mathcal{I},s_\mathcal{G})$ that only uses actions in $A_\epsilon$: in particular for the full triple $(s_\mathcal{I},s_\mathcal{G},T)$, the (unknown) $T$ uses an action outside $A_\epsilon$ with probability at most $|A|\cdot\frac{\epsilon}{2|A|}=\epsilon/2$.
	Likewise since the preconditions are adequate for $A_\epsilon$, with probability $1-\epsilon/2$ the specified preconditions are satisfied on all of the states in which the corresponding action is used in the unknown trajectory; thus, by a union bound, we find that with probability $1-\epsilon$, the plan in the trajectory is a solution to the planning task in which the actions are restricted to $A_\epsilon\subseteq A(\mathcal{T})$.
	Moreover, since the assignment of preconditions to actions is safe (Theorem~\ref{the:safeness}), if we only use actions in $A(\mathcal{T})$, whenever our precondition for $a\in A(\mathcal{T})$ is satisfied for a state in our trajectory, the real precondition is also satisfied. 
\end{proof}





contains an action tr
By Moreover, the probability of sampling a trajectory 
It has a set of state and action pairs that 

Our algorithm may output this inadequate action model if 
the given trajectories $\mathcal{T}$ do not contain any action triple ($s,a,s'$) 


Since the action model is inadequate, then with probability $\epsilon/2$ 
we will draw a trajectory that contains an action triple ($s,a,s'$) 
in which $a\in A_\epsilon$ and it cannot be applied 


it has a set of actions $A_{BAD}\subseteq A_\epsilon$ for which 

This assignment cannot be outputted by our algorithm 
We note that the action model we learn cannot be not adequate 

any particular inadequate assignment cannot be produced as output by the algorithm if we draw a trajectory in which for one of the actions in $a\in A_\epsilon$ the precondition assigned to $a$ by that inadequate assignment is not satisfied on one of the states in the trajectory in which $a$ is invoked. 
Consider the probability that we have learned an action model with a precondition assignment from this set ($BAD$). 

By the definition of adequacy, if the action model is adequate then 
we will 
i state in which an action cannot be applied appears in a trajectory with probability at least $\epsilon/2$. 

in each example trajectory drawn, 


such a state appears with probability at least $\epsilon/2$. Therefore, the probability that no state appears in our $m$ independently drawn trajectories is at most $(1-\epsilon/2)^m$. Concretely, since $m\geq\frac{(2ln 2)|A|}{\epsilon}(|P|+\log\frac{2|A|}{\delta})>\frac{2\ln 2}{\epsilon}(|A||P|+\log\frac{2}{\delta})$, this is at most $\delta/2^{|A||P|+1}$. Thus, by a union bound over this set of inadequate assignments $BAD$, the probability that any inadequate assignment of preconditions could be output is at most $\delta/2$.



We note that any particular inadequate assignment cannot be produced as output by the algorithm if we draw a trajectory in which for one of the actions in $a\in A_\epsilon$ the precondition assigned to $a$ by that inadequate assignment is not satisfied on one of the states in the trajectory in which $a$ is invoked. Note that by the definition of inadequacy, for each example trajectory drawn, such a state appears with probability at least $\epsilon/2$. Therefore, the probability that no state appears in our $m$ independently drawn trajectories is at most $(1-\epsilon/2)^m$. Concretely, since $m\geq\frac{(2ln 2)|A|}{\epsilon}(|P|+\log\frac{2|A|}{\delta})>\frac{2\ln 2}{\epsilon}(|A||P|+\log\frac{2}{\delta})$, this is at most $\delta/2^{|A||P|+1}$. Thus, by a union bound over this set of inadequate assignments $BAD$, the probability that any inadequate assignment of preconditions could be output is at most $\delta/2$.



